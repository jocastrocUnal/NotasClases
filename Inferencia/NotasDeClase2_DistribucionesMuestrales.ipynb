{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fa4d39-a4f8-4eee-bd15-58e389f44886",
   "metadata": {},
   "source": [
    "# **Distribuciones Muestrales**\n",
    "Vamos a ver algunas propiedades y características de ciertos valores que se miden sobre la muestra. Sea una [muestra aleatoria](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#DefinicionMuestraAleatoria) $X_{1},X_{2},\\cdots,X_{n}$ tenemos que sobre esta muestra aleatoria podemos definir otra variable aleatoria con algunas estadísticas o cantidades muestrales de uso frecuentes como la media muestral $\\overline{X}_{n}$, la varianza muestral $S_n^2$; y algunas de orden como la mediana $\\mu_e$, los percentiles $X_p$ con $0 \\leq p \\leq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5d2a5-8241-4dc9-989e-09c3f169cbbe",
   "metadata": {},
   "source": [
    "<a id='DefinicionMediaMuestral'></a>\n",
    "### **Media Muestral**\n",
    "Sean $X_1,X_2,\\cdots,X_n$ una colección de variables aleatorias. Se define a la media muestral como \n",
    "\n",
    "$$\n",
    "\\overline{X}_n := \\frac{X_1+X_2+\\cdots+X_n}{n}\n",
    "$$\n",
    "\n",
    "<a id='TeoremaPropiedadesMediaMuestral'></a>\n",
    "#### **Propiedades de la media muestral**\n",
    "Sean $X_1,X_2,\\cdots,X_n$ una \n",
    "[muestra aleatoria](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#DefinicionMuestraAleatoria)\n",
    "con media $\\mu$ y varianza $\\sigma^2$ finitas. Entonces:\n",
    "\n",
    "1) $E[\\overline{X}_n] = \\mu$\n",
    "\n",
    "2) $\\operatorname{Var}[\\overline{X}_n] = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "3) Si $X_i$ con $1 \\leq i \\leq n$ es tal que $X_i \\sim N(\\mu,\\sigma^2)$, entonces\n",
    "$$\n",
    "\\overline{X}_n \\sim N \\left( \\mu , \\frac{\\sigma^2}{n} \\right)\n",
    "$$\n",
    "\n",
    "- De no tener la independencia que nos da la muestra aleatoria se sigue cumpliendo la propiedad 1) pero la 2) cambia agregando un factor que tiene en cuenta la correlación.\n",
    "\n",
    "<a id='TeoremaPropiedadesAsintoticasMediaMuestral'></a>\n",
    "#### **Propiedades asintóticas de la media muestral** \n",
    "\n",
    "<a id='TeoremaLeyDebilGrandesNumeros'></a>\n",
    "##### **ley débil de los grandes números**\n",
    "Sean $X_1,X_2,\\cdots,X_n$ una muestra aleatoria con media $\\mu$ y varianza $\\sigma^2$ finitas. Y sea \n",
    "\n",
    "$$\n",
    "\\overline{X}_n = \\frac{X_1+X_2+\\cdots+X_n}{n}\n",
    "$$\n",
    "\n",
    "Entonces $\\overline{X}_n \\overset{p}{\\underset{n\\to\\infty}{\\longrightarrow}} \\mu$\n",
    "\n",
    "-  De esta definición viene la interpretación que dábamos al valor esperado.\n",
    "\n",
    "- En cierto modo nos dice que en el límite no va a haber variabilidad y que muy probablemente vamos a estar cerca del valor esperado.\n",
    "\n",
    "- Hay generalizaciones que no requieren que la varianza sea finita.\n",
    "\n",
    "- ***(Dem 202125 0:25:00)***\n",
    "\n",
    "> Es importante tener en cuenta que la ley de los grandes números quiere decir que los promedios y las proporciones o frecuencias, se vuelven valores esperados y probabilidades cuando se tiende a infinito.\n",
    "\n",
    "<a id='TeoremaLeyFuerteGrandesNumeros'></a>\n",
    "##### **Ley fuerte de los grandes números**\n",
    "\n",
    "Sean $X_1,X_2,\\cdots,X_n$ una muestra aleatoria con media $\\mu$ y varianza $\\sigma^2$ finitas. Y sea \n",
    "\n",
    "$$\n",
    "\\overline{X}_n = \\frac{X_1+X_2+\\cdots+X_n}{n}\n",
    "$$\n",
    "\n",
    "Entonces $\\overline{X}_n \\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \\mu$\n",
    "\n",
    "- Un resultado más fuerte\n",
    "\n",
    "<a id='TeoremaLimiteCentral'></a>\n",
    "##### **Teorema del límite central**\n",
    "Sean $X_1,X_2,\\cdots,X_n$ una muestra aleatoria con media $\\mu$ y varianza $\\sigma^2$ finitas. Y sea \n",
    "\n",
    "$$\n",
    "\\overline{X}_n = \\frac{X_1+X_2+\\cdots+X_n}{n} \\ \\ , \\ \\ \\ T_n=\\sum_{j=1}^{n} X_j\n",
    "$$\n",
    "\n",
    "Entonces\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{1) } \\frac{T_n-n\\mu}{\\sigma\\sqrt{n}} \\overset{d}{\\underset{n\\to\\infty}{\\longrightarrow}} N(0,1) \n",
    "& & & T_n \\approx N(n\\mu , n\\sigma^2) \\\\\n",
    "\\text{2) } \\sqrt{n}\\frac{\\overline{X}_n-\\mu}{\\sigma} \\overset{d}{\\underset{n\\to\\infty}{\\longrightarrow}} N(0,1)\n",
    "& & & \\overline{X}_n \\approx N(\\mu,\\frac{\\sigma^2}{n})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Los resultados a la derecha del teorema, son no formales desde el punto de vista probabilístico e incluso matemático, dado que podria entenderse como $n \\approx n+1$. Pero si están asociados a la forma como se aplican estos teoremas desde una reinterpretacion de los límites de la izquierda para cuando tenemos un $n$ lo suficientemente \"grande\".\n",
    "\n",
    "- No importa de qué distribución venga la muestra aleatoria. El promedio muestra y la suma se distribuirán normales cuando el tamaño de muestra sea \"grande\".\n",
    "\n",
    "- Si la muestra aleatoria es normal, el resultado se tiene de manera exacta.\n",
    "\n",
    "- *\"La distribución uniforme es latikurtica vamos a ver como esa platokurtosis se va transformando en lectokurtosis para volverse como una distribución normal\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f1284-11c1-4d3c-a64c-9637366265d8",
   "metadata": {},
   "source": [
    "#### Ejemplo 4\n",
    "Sea $\\{ X_n \\}_{n \\in \\mathbb{Z}^+}$ una muestra aleatoria tal que $E[X_1^r]$ existe para todo $r \\in \\mathbb{Z}^+$ (*lo que implica por la definición de muestra aleatoria que $E[X_i^r]$ existe para todo $r \\in \\mathbb{Z}^+$*). Muestre que \n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{j=1}^n X_j^r \\overset{p}{\\underset{n\\to\\infty}{\\longrightarrow}} E[X_1^r]\n",
    "$$\n",
    "\n",
    "Podemos mostrar este resultado tambien haciendo o la [definición de convergencia en probabilidad de una variable aleatoria](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#ConvergenciaProbabilidad) o bien alguna de las [propiedades de convergencia](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#PropiedadesConvergencia).\n",
    "\n",
    "Vamos a hacer uso de las [propiedades asintóticas de la media muestral](#TeoremaPropiedadesAsintoticasMediaMuestral).\n",
    "\n",
    "Sea $\\{ Y_n \\}_{n \\in \\mathbb{Z}^+}$ una muestra aleatoria tal que $Y_i=X_i^r$ (*tenemos que tener en cuenta que si las $X_i$ son independientes, $X_i^r$, para cualquier $r$ que puedan llegar a tener, tambien son independientes y tambien tienen la misma distribución*). Tambien desde el enunciado tenemos que $E[Y_i]=k$ es finito. De este modo podemos aplicar la [ley debil de los grandes números](#TeoremaLeyDebilGrandesNumeros) a partir de que\n",
    "\n",
    "$$\n",
    "\\overline{Y}_n = \n",
    "\\frac{1}{n} \\sum_{i=1}^{n} Y_i =\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} X_i^r \n",
    "$$\n",
    "\n",
    "De este modo teniendo ya que \n",
    "\n",
    "$$\n",
    "\\overline{Y}_n \\overset{p}{\\underset{n\\to\\infty}{\\longrightarrow}} k\n",
    "$$\n",
    "\n",
    "remplazando nos deja que\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} X_i^r \\overset{p}{\\underset{n\\to\\infty}{\\longrightarrow}} E[\\overline{X}_i^r]\n",
    "$$\n",
    "\n",
    "En particular\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} X_i^r \\overset{p}{\\underset{n\\to\\infty}{\\longrightarrow}} E[\\overline{X}_1^r]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a38304-c18d-4c2b-948f-0ee7b001c867",
   "metadata": {},
   "source": [
    "#### Ejemplo 5\n",
    "Sea $\\{ X_n \\}_{n \\in \\mathbb{Z}^{+}}$ una muestra aleatoria tal que $E[X_1]=\\mu$ y $\\operatorname{Var}[X_1]= \\sigma^2$ son menores que $\\infty$ (*existen*). Muestre que \n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{j=1}^{n}X_j}{\\sqrt{n\\sum_{j=1}^n X_j^2}}\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \n",
    "\\frac{\\mu}{\\sqrt{\\mu^2+\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Teniendo en cuenta la [ley fuerte de los grandes números](#TeoremaLeyFuerteGrandesNumeros) tenemos que \n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{j=1}^{n}X_j\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \n",
    "\\mu\n",
    "$$\n",
    "\n",
    "Hagamos un producto de $1=\\frac{\\frac{1}{n}}{\\frac{1}{n}}$ y haciendo uso de las [propiedades de la convergencia](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#PropiedadesConvergencia) veamos que tenemos\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{j=1}^{n}X_j\\frac{1}{\\frac{1}{n}\\sqrt{n\\sum_{j=1}^n X_j^2}}\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \n",
    "\\mu\\frac{1}{\\sqrt{\\mu^2+\\sigma^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{j=1}^{n}X_j\\frac{1}{\\sqrt{\\frac{1}{n}\\sum_{j=1}^n X_j^2}}\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \n",
    "\\mu\\frac{1}{\\sqrt{\\mu^2+\\sigma^2}}\n",
    "$$\n",
    "\n",
    "lo que corresponde que en el numerador tengamos\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{j=1}^{n}X_j\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \n",
    "\\mu \\ \\ \\ \\ \\text{ (1)}\n",
    "$$\n",
    "\n",
    "y lo que deberia ser para el denominador\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{j=1}^n X_j^2}\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \n",
    "\\sqrt{\\mu^2+\\sigma^2}  \\ \\ \\ \\ \\text{ (2)}\n",
    "$$\n",
    "\n",
    "Vamos a probar (2). Veamos a que converge casi seguramente $\\sqrt{\\frac{1}{n}\\sum_{j=1}^n X_j^2}$. Tengamos en cuenta la definición la propiedad de la varianza que nos dice $\\operatorname{Var}=E[Z-E(Z)]^2=E[Z^2]-E[Z]^2$, luego tenemos que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^2 & = E[X_j^2] - \\mu^2 \\\\\n",
    "\\mu^2 + \\sigma^2 & = E[X_j^2] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "lo que significa que el valor esperado de cada $E[X_j^2]$ existe y es $\\mu^2 + \\sigma^2$, dado que por definición $\\mu$ y $\\sigma$ existen para cada $X_j$. Entonces tenemos que \n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{j=1}^n X_j^2\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}}\n",
    "\\mu^2 + \\sigma^2\n",
    "$$\n",
    "\n",
    "Pero como por las mismas [propiedades de la convergencia](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#PropiedadesConvergencia) tenemos que para $f(x)=\\sqrt{x}$, una función continua en el soporte de $X_i$ y por tanto de $X_i^2$, para el denominador tenemos que \n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{j=1}^n X_j^2}\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}}\n",
    "\\sqrt{\\mu^2 + \\sigma^2}\n",
    "$$\n",
    "\n",
    "Por lo tanto es cierto que (*\"siempre y cuando $\\sigma$ no sea cero, porque entonces este denominador podría irse a cero\"*)\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{j=1}^{n}X_j}{\\sqrt{n\\sum_{j=1}^n X_j^2}}\n",
    "\\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \n",
    "\\frac{\\mu}{\\sqrt{\\mu^2+\\sigma^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8d4e1-4d4b-4b46-951c-ec33c113f382",
   "metadata": {},
   "source": [
    "#### Ejemplo 6\n",
    "Se recolectó una muestra de tamaño 16 de los productos terminados de una compañía para evaluar si había o no imperfectos.\n",
    "\n",
    "- ¿Qué modelo probabilístico usaría par amodelar la presencia de imperfectos en cada uno de los productos?\n",
    "\n",
    "Recordemos que para algunas variables aleatorias de tipo dicotómicas (el individuo esta en una categoría o no) generalmente se modela apartir de una [distribucion de tipo Bernoulli](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#DistribuciónBernoulli), de este modo tenemos que $X_i$ es una variable aleatoria tal que $X_i=$ \"El $i$-ésimo producto tiene imperfectos\", de este modo $X_i \\sim Ber(p)$ con $1 \\leq i \\leq 16$ es decir\n",
    "\n",
    "$$\n",
    "X=\n",
    "\\begin{cases}\n",
    "    1 & \\text{ , Si el producto está defectuoso} \\\\\n",
    "    0 & \\text{ ,  en otro caso} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Es importante tener entonces cuenta que $X_1,X_2,\\cdots,X_{16}$ deben formar una [muestra aleatoria](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#DefinicionMuestraAleatoria) y debemos suponer por tanto que los defectos del algún producto $X_i$ no determina los defectos de otro producto diferente al el mismo.\n",
    "\n",
    "\n",
    "- ¿Cuál es la probabilidad de que la proporción de imperfectos en la muestra dista más de 10% de la proporción de imperfecto en la población? ¿Se puede hacer el cálculo exacto?\n",
    "\n",
    "Recordemos que el parámetro $p$ de nuestra distribución \"solo lo conocen algunos seres supremos que han visto sobre toda la población\" y nosotros pretendemos es buscar un [estimador](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#PrincipioEstimacion) el cual nos de el porcentaje de defectuosos en la muestra.\n",
    "\n",
    "De este modo como tengo una muestra aleatoria del tipo Bernoulli esta tiene media $\\mu=p$ y varianza $p(1-p)$ con $0<p<1$. Luego si queremos en un principio saber cual es el porcentaje de defectuosos en muestra tenemos que buscar el promedio $\\overline{X}_{16}$ ; ya que desde su definición, $\\overline{X}_{16}=\\frac{X_1,X_2,\\cdots,X_{16}}{16}$, tenemos que en el numerador el total de porductos defectuoso y en el denominador el número total de productos observados, lo que recalca que los promedios en las variables de tipo Bernoulli son proporciones.\n",
    "\n",
    "Por lo tanto lo que nos estan preguntando es a que es igual $p(|\\overline{X}_{16} - p| > 0.1=10\\%)$. Sabemos eso si que $\\overline{X}_n \\overset{p}{\\underset{n \\to \\infty}{\\longrightarrow}} p$ pero imposible llegar a que $n\\to\\infty$. Luego en ocasiones tenemos que hacer un compromiso aplicable en decir cual realmentes es un $n$ lo suficientemente grande, o dado el $n$ que tenemos cual es la probabilidad de estar lo suficientemente alejado $\\overline{X}_n$ de $p$. Está probabilidad no la podemos conocer de manera exacta dado que no sabemos cuanto es $p$ y ademas de $\\overline{X}_n$ no conocemos su distribución y siendo así es seguro que depende de $p$ igualmente. Entonces NO podemos dar un resultado exacto pero si podemos dar un resultado aproximado; para esto vamos a hacer uso del [Teorema del Límite Central](#TeoremaLimiteCentral).\n",
    "\n",
    "- Ahora use el teorema central del límite para obtener una solución aproximada. ¿Cómo le explicaría ese resultado a la compañía?\n",
    "\n",
    "Recordando el [Teorema del Límite Central](#TeoremaLimiteCentral) tenemos que es plausible aproximar para una muestra aleatoria $X_1,X_2,\\cdots,X_{n}$ donde su valor esperado y varianza existen que $\\overline{X}_n \\approx N(\\mu,\\frac{\\sigma^2}{n})$. De este modo para nuestro el caso de la distribución de nuestra muestra tenemos que $\\overline{X}_n \\approx N(p,\\frac{p(1-p)}{n})$ dado que $\\sqrt{n}\\frac{\\overline{X}_n-p}{\\sqrt{p(1-p)}} \\overset{d}{\\underset{n \\to \\infty}{\\longrightarrow}} N(0,1)$\n",
    "\n",
    "Entonces \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(|\\overline{X}_{16} - p| > 0.1) \n",
    "& = p(\\overline{X}_{16} - p < -0.1 \\text{ ó } \\overline{X}_{16} - p > 0.1) \\\\\n",
    "& = p(\\overline{X}_{16} - p < -0.1) + p(\\overline{X}_{16} - p > 0.1) \\\\\n",
    "& = p\\left( \\sqrt{16}(\\overline{X}_{16} - p) < \\sqrt{16}(-0.1) \\right) + p\\left( \\sqrt{16}(\\overline{X}_{16} - p) > \\sqrt{16}(0.1) \\right) \\\\\n",
    "& = p\\left( \\frac{\\sqrt{16}(\\overline{X}_{16} - p)}{\\sqrt{p(1-p)}} < \\frac{-\\sqrt{16}(0.1)}{\\sqrt{p(1-p)}} \\right) + p\\left( \\frac{\\sqrt{16}(\\overline{X}_{16} - p)}{\\sqrt{p(1-p)}} > \\frac{\\sqrt{16}(0.1)}{\\sqrt{p(1-p)}} \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Sea $Z \\sim N(0,1)$ a pesar de que seguimos sin librarnos de $p$ es importante notar que en el peor de los casos tenemos que $p=0.5$ ya que es el momendo donde ella presenta más varianza. Esto lo podemos ver en la siguiente representación\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/nY8zF63b8jDUz2U5sMPyPaKedeRZ1Y-cSYstBXzIxlg.original.fullsize.png)\n",
    "\n",
    "Entonces es lo que llamaremos una sobrestimación de $p$. Tambien para el caso podriamos tomar $p$ como el resultado de $\\overline{X}_{16}$ lo cual llamaremos una muestra piloto.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p\\left( \\frac{\\sqrt{16}(\\overline{X}_{16} - p)}{\\sqrt{p(1-p)}} < \\frac{-\\sqrt{16}(0.1)}{\\sqrt{p(1-p)}} \\right) + p\\left( \\frac{\\sqrt{16}(\\overline{X}_{16} - p)}{\\sqrt{p(1-p)}} > \\frac{\\sqrt{16}(0.1)}{\\sqrt{p(1-p)}} \\right)\n",
    "& \\approx p(Z<-0.8)+p(Z>0.8) \\\\\n",
    "& = 2p(Z>0.8) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Esto porque en la estandarización tenemos $0.8 = \\frac{4*0.1}{0.5} = \\frac{\\sqrt{16}*0.1}{\\sqrt{0.5(1-0.5)}} = \\frac{\\sqrt{n}*0.1}{\\sqrt{p(1-p)}}$ y Z tiene una distribución simétrica; lo cual nos da "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6a21dc-98f0-4994-8288-39959259c143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4237107971667933"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import *\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Evaluacion de 0.8 en una inversa de una función de distribución acumulada de una normal estandar\n",
    "p_Z_Ber_n16 = 1 - norm.cdf(0.8, loc =0, scale = 1) \n",
    "2*p_Z_Ber_n16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b285760-18a0-428f-8ad6-46023b33ecc2",
   "metadata": {},
   "source": [
    "Por lo tanto lo que tenemos es que en el peor de los casos, para el tamaño de muestra que tomamos, la probabilidad aproximada según el tamaño de la muestra de que el número de defectos encontrados en la muestra diste del valor real de defectos $p$ en más de un 10%, es del 42.37%. Lo cual es bastante considerable ya que casi la mitad de las muestras que tomemos van a fallar en en estar en un 10% cerca a la media $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100ff48-2512-4cf6-bfc9-2521c4e697cc",
   "metadata": {},
   "source": [
    "- ¿Cuál debería ser el tamaño de muestra para que dicha probabilidad no supere el 1%?\n",
    "\n",
    "    - Use el [teorema del límite central](#TeoremaLimiteCentral)\n",
    "    \n",
    "Primero veamos recordemos que, en la parte anterior, usamos el valor del 10% (0.1) al hacer la estandarización y obtuvimos un error de más del 42%, de este modo recopilemos tanto el análisis anterior como su resultado pero esta vez dejando como incognita a $n$ y esperando a 1% (0.01) como la probabilidad de que $p(|\\overline{X}_{n} - p| > 0.1=10\\%)$. Entonces\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p\\left( \\frac{\\sqrt{n}(\\overline{X}_{n} - p)}{\\sqrt{p(1-p)}} < \\frac{-\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\right) + p\\left( \\frac{\\sqrt{n}(\\overline{X}_{n} - p)}{\\sqrt{p(1-p)}} > \\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\right)\n",
    "& \\\\\n",
    "\\approx \n",
    "p\\left( Z < \\frac{-\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\right) + p\\left( Z > \\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\right) \\\\\n",
    "= 2p\\left( Z>\\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\right) = 0.01 \\\\\n",
    "= p\\left( Z>\\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\right) = \\frac{0.01}{2} \\\\\n",
    "= p\\left( Z>\\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\right) = 0.005 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "De este modo tenemos ahora que encontrar cuantiles de la normal estandar (La funcón que da los cuantiles es $Q=F^{{-1}}$ donde $F$ es la función de distribución [Ref](https://en.wikipedia.org/wiki/Quantile_function).). Es decir tenemos que encontrar valores de Z para los cuales $P(Z>\\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}})= 0.005$, esto es lo mismo que decir $P(Z \\leq \\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}})= 0.995$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140cca15-a1fd-44f4-80d5-191ef540dccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5758293035489004"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import *\n",
    "from scipy.stats import norm\n",
    "\n",
    "norm.ppf(0.995, loc =0, scale = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021581c6-ff4e-4b91-ac4e-2990703a3e46",
   "metadata": {},
   "source": [
    "De este modo tenemos que $\\frac{\\sqrt{n}(0.1)}{\\sqrt{p(1-p)}} \\approx 2.57583 = Z_{0.995}$. En este punto es necesario hacer nuevamente una suposición para $p$ donde nuevamente vamos a tomar el peor escenario para este que es $p=0.5$. Así para encontrar $n$ tenemos\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\sqrt{n}(0.1)}{0.5} & = 2.57583\\\\\n",
    "\\sqrt{n}(0.1) & = 2.57583*0.5 \\\\\n",
    "\\sqrt{n} & = \\frac{2.57583*0.5}{0.1} \\\\\n",
    "n & = \\left( \\frac{2.57583*0.5}{0.1} \\right)^2 \\\\\n",
    "n & = 165.87 \\approx 166 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Use la [desigualdad de Chebychev](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#TeoremaDesigualdadChebychev)\n",
    "\n",
    "Según la desigualdad de Chebychev tenemos que \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(|X-\\mu_X| \\leq \\varepsilon) \\geq 1 - \\frac{\\sigma^2}{\\varepsilon^2} \n",
    "& \\to p(|\\overline{X}_n-p| \\leq \\varepsilon) \\geq 1 - \\frac{\\frac{p(1-p)}{n} }{\\varepsilon^2} \\\\\n",
    "& = p(|\\overline{X}_n-p| \\leq \\varepsilon) \\geq 1 - \\frac{p(1-p)}{n\\varepsilon^2} \\\\\n",
    "& = p(|\\overline{X}_n-p| > \\varepsilon) \\leq 1 - \\frac{p(1-p)}{n\\varepsilon^2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "dado que $\\operatorname{Var}\\overline{X}_n = \\frac{\\sigma^2}{n}$ como vimos en las [propiedades de la media muestral](#TeoremaPropiedadesMediaMuestral). De este para nuestro caso quisieramos ver la cota superior de $p(|\\overline{X}_n-p| > \\varepsilon)$ tal que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(|\\overline{X}_n-p| > \\varepsilon) \\leq 1 - \\frac{p(1-p)}{n\\varepsilon^2}\n",
    "& = p(|\\overline{X}_n-p| > 0.1) \\leq 1 - \\frac{p(1-p)}{n(0.1)^2} = 0.01 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Nuevamente tenemos que volver a suponer $p=0.5$ y tendriamos que esa cota superior no es más que $\\frac{0.25}{n(0.1)^2}=0.01$ y de este modo $n=2500$\n",
    "\n",
    "\n",
    "- compare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73134aef-6115-4d4f-8f41-0b3924b22979",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Desde el punto de vista de estadística descriptiva, la media $\\overline{x}_n$ se puede interpretar como una medida de tendencia central o nos trata decir donde se agrupan los datos (*note que se dijo $x_n$ y no la variable aleatoria $\\overline{X}_{n}$*); varianza $s^2$, por otro lado, se puede interpretar como una medida de disperción de los datos. Recordemos tambien que es usual en estadística descriptiva se advierta que **la media y la varianza son bastante sensibles cuando se tienen datos atípicos**. Dado que las variables aleatorias $\\overline{X}_n$ y $S_n^2$ (*varianza Muestral*) son nuestro \"caballito de batalla\" en inferencia estadística, tenemos que tener siempre presente este hecho tambien. \n",
    "\n",
    "> Lo que nos queda ver por ahora es, que se puede hacer cuando se tienen datos atípicos. Un primer paso dentro del proceso de inferencia es ver cuales son esos datos atípicos, para lo cual podemos usar herramientas como el boxplott (*caja y bigotes*). Segundo tenemos que decidir si los eliminamos siempre y cuando provengan de un error de medida o imputo datos para estabilizar; o si nos ponemos a la tarea de entender porque se generan esos datos atípicos i.e. que la misma naturaleza de los datos esta generando una \"atipicidad\" en la medida. Tambien es usal que dada nuestra suposición de tener una muestra aleatoria no sea el enfoque correcto al problema, luego tambien cabe la posibilidad que el enfoque al problema genere datos atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df9173-fce9-48d6-b4f0-169ac8e00cc7",
   "metadata": {},
   "source": [
    "<a id='DefinicionVarianzaMuestral'></a>\n",
    "### **Varianza Muestral**\n",
    "Sean $X_1,X_2,\\cdots ,X_n$ una muestra aleatoria y $\\overline{X}_n$ la [media muestral](#DefinicionMediaMuestral) definida a partir de la muestra aleatoria. Se define la varianza muestral como \n",
    "$$S_n^2 := \\frac{1}{n-1} \\sum_{i=1}^n (X_i- \\overline{X}_n)^2$$\n",
    "\n",
    "*Note que la varianza muestral se define sobre $\\frac{1}{n-1}$ y no sobre $\\frac{1}{n}$ dado que la varianza expresada de esa manera entre tantas propiedades, en especial que, $E[S_n^2]=\\sigma^2$.*\n",
    "\n",
    "<a id='TeoremaPropiedadesVarianzaMuestral'></a>\n",
    "### **Propiedades de la Varianza Muestral**\n",
    "Sean $X_1,X_2,\\cdots ,X_n$ una muestra aleatoria con media $\\mu$ y varianza $\\sigma^2$ finitas. Entonces:\n",
    "\n",
    "1) $E[S_n^2]=\\sigma^2$ \n",
    "\n",
    "*Esto quiere decir que la varianza es el centro de masa de la variable aletoria $S_n^2$ (en tendencia). Tambien es útil a menudo preguntarse que para todo caso si $E[S^2_n]=\\sigma^2$ ent $E[S_n]=\\sigma$; digamos que siempre que $S^2_n$ es una variable aleatoria constante o si $n$ tiende a $\\infty$. Pero dado que el valor esperado de una funcion no es igual, a la función de un valor esperado si $f$ no es lineal; entonces $E[f(x)] = f(E[x])$ siempre que $f$ sea una función lineal.*\n",
    "\n",
    "2) Además, si la muestra aleatoria tiene una distribución simétrica con segundos momentos finitos, se tiene que $\\operatorname{Cov}(\\overline{X}_n,S_n^2)=0$ \n",
    "\n",
    "*Lo que significa que la covarianza sea cero es que no se tiene una relación lineal entre las dos variables aleatorias. Un resultado de la probabilidad es que la independencia entre variables implica $\\operatorname{Cov}(x,y)=0$ y $\\rho(x,y)=0$ (covarianza y correlacion), pero la otra implicación no se asegura dado que la covarianza y la correlación mide relaciónes lineales, asi puede que la varianza este logaritmicamente relacionada con la media pero no linealmente relacionada con la media. El único caso para el que esto si se tiene la dobre implicación es cuando $X,Y \\sim NMV(\\mu_{X,Y},\\Sigma_{X,Y})$ (Mayorga)*\n",
    "\n",
    "*Este resultado es útil dado que al tener unas variables aleatorias $X_{1},X_{2},\\cdots,X_{n}$ si por ejemplo $X_i \\sim P(\\lambda)$ se que $E[\\overline{X}_i]=\\lambda$ y que $E[S_n^2]=\\lambda$ luego $\\operatorname{Cor}(\\overline{X}_n,S_n^2)>0$. Si por otro lado $X_i \\sim N(\\mu,\\sigma^2)$ tenemos que $E[\\overline{X}_i]=\\mu$ y que $E[S_n^2]=\\sigma^2$ luego $\\operatorname{Cor}(\\overline{X}_n),S_n^2)=0$ *\n",
    "\n",
    "3) Si la muestra aleotaria es normal, se tiene que:\n",
    "\n",
    "    1) $\\overline{X}_n$ y $S_n^2$ son variables aleatorias independientes. ***(Dem 0:53:00)***\n",
    "    \n",
    "    2) $\\frac{(n-1)S_n^2}{\\sigma^2} \\sim \\chi^2(n-1)$.\n",
    "    \n",
    "    3) $\\frac{\\sqrt{n}(\\overline{X}_n-\\mu)}{S_n} \\sim t(n-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55047f8b-4376-4113-ac82-5991dd19518d",
   "metadata": {},
   "source": [
    "<a id='TeoremaPropiedadesAsintoticasVarianzaMuestral'></a>\n",
    "#### **Propiedades asintóticas de la varianza**\n",
    "\n",
    "Sean $X_1, X_2,\\cdots ,X_n$ una muestra aleatorias con media $\\mu$ y varianza $\\sigma^2$ finitas. Entonces:\n",
    "\n",
    "1) $S_n^2 \\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \\sigma^2$\n",
    "\n",
    "2) $S_n \\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \\sigma$\n",
    "\n",
    "3) $\\frac{\\sqrt{n}(\\overline{X}_n-\\mu)}{S_n} \\overset{d}{\\underset{n\\to\\infty}{\\longrightarrow}} N(0,1)$ (***Dem con el teorema de slutsky***)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2b2da-b15e-43f9-9091-f6bd9aa6e829",
   "metadata": {},
   "source": [
    "<a id='DefincionPercentilesVariableAleatoria'></a>\n",
    "### **Percentiles de una variable aleatoria**\n",
    "\n",
    "#### **Percentil de una variable aleatoria**\n",
    "El percentil de $\\alpha$ ($0 \\leq \\alpha \\leq 1$) de una variable aleatoria $X$ es un número real $x_a$ tal que \n",
    "\n",
    "$$\n",
    "F_x(x_a) = \\alpha\n",
    "$$\n",
    "\n",
    "#### **Percentil de una variable aleatoria**\n",
    "El percentil de $\\alpha$ ($0\\leq\\alpha\\leq1$) de una variable aleatoria $X$ es todo número real $x_a$ tal que:\n",
    "\n",
    "$$\n",
    "F_x(x_a) \\geq \\alpha \\quad p(X\\geq x_a) \\geq 1-\\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60b508-e849-4633-be5e-ed0cdc3fdcf7",
   "metadata": {},
   "source": [
    "<a id='DefinicionEstadisticasOrden'></a>\n",
    "### **Estadísticas de orden**\n",
    "Sean $X_{1},X_{2},\\cdots,X_{n}$ una colección de variables aleatoriass. Se define el vector de estadísticas de orden como $X^{(1)},X^{(2)},\\cdots,X^{(n)}$ donde \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X^{(1)} & := \\operatorname{min}\\{ X_{1},X_{2},\\cdots,X_{n} \\} \\to X^{(1)} \\overset{c.s.}{\\leq} X^{(i)} \\quad \\forall i \\\\\n",
    "X^{(2)} & := \\exists ! X_{k} \\leq X_{2} , \\quad \\text{pero} \\quad X^{(2)} \\overset{c.s.}{\\leq} X^{(i)} \\quad \\forall i \\\\\n",
    "\\vdots & \\quad \\quad \\vdots \\\\\n",
    "X^{(n)} & := \\operatorname{max}\\{ X_{1},X_{2},\\cdots,X_{n} \\} \\to X^{(n)} \\overset{c.s.}{\\geq} X^{(i)} \\quad \\forall i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Las estadísticas de orden son el equivalente a los percentiles en la población, se definen tambien como una variable aleatoria y funcionan como un buen estimador.*\n",
    "\n",
    "<a id='DefinicionMedianaMuestral'></a>\n",
    "#### **Definición de la mediana**\n",
    "Sea $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria y $X^{(1)},X^{(2)},\\cdots,X^{(n)}$ las estadísticas de orden asociadas. La mediana muestral se define como\n",
    "$$\n",
    "\\mu_e =\n",
    "\\begin{cases}\n",
    "\\frac{X^{(\\frac{n}{2})}+X^{(\\frac{n}{2}+1)}}{2} & \\text{si $n$ es par} \\\\\n",
    "X^{(\\lfloor \\frac{n}{2} \\rfloor + 1)} & \\text{si $n$ es impar} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "donde $\\lfloor q \\rfloor$ es la función parte entera de $q$. \n",
    "\n",
    "*De este modo tenemos que la mediana es aquel valor sobre las estadísitcas de orden que deja el 50% de los valores a su izquierda y a derecha. A menudo es muy útil si se tienen valores atípicos en la muestra.*\n",
    "\n",
    "<a id='TeoremaDistribucionEstadisticasOrden'></a>\n",
    "#### **Distribución de una estadística de orden**\n",
    "Sean $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria con $f$. de distribución $F_X$. Entonces\n",
    "\n",
    "$$\n",
    "F_{X^{k}} = \\sum_{j=k}^{n} \\binom{n}{j}[F_X(x)]^j[1-F_x(x)]^{n-j} \\quad , \\quad \\forall x \\in D_X\n",
    "$$\n",
    "\n",
    "#### Ejemplo\n",
    "Sea $X_{1},X_{2},\\cdots,X_{n}$ una muesta aleatoria tal que $X_i \\sim Exp(\\lambda)$. Dado que \n",
    "\n",
    "$$\n",
    "F_X(x)=1-e^{-\\lambda x}= P(X \\leq x)\n",
    "$$\n",
    "\n",
    "donde entonces\n",
    "\n",
    "$$\n",
    "P(X>x)=1-P(X \\leq x)=e^{-\\lambda x}\n",
    "$$\n",
    "\n",
    "para $X^{(1)} := \\operatorname{min}\\{ X_{1},X_{2},\\cdots,X_{n} \\}$ la $P(X^{1}>x) \\quad \\forall x \\in \\mathbb{R}^+ = D_X$ tenemos\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(X^{(1)}>x) \n",
    "& = P(X_{1}>x,X_{2}>x,\\cdots,X_{n}>x) \\\\\n",
    "& = P(X_{1}>x) p(X_{2}>x)\\cdots P(X_{n}>x) \\\\\n",
    "& = e^{-\\lambda x} e^{-\\lambda x} \\cdots e^{-\\lambda x} \\\\\n",
    "& = e^{-n\\lambda x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "De esta manera probamos que el mínimo de las muestras de los exponenciales tiene distribución exponencial $X^{(1)} \\sim Exp(n\\lambda)$\n",
    "\n",
    "> De este modo podemos ver que a las variables aleatorias asociadas a las estadísticas de orden les puedo encontrar su distribución a partir de la distribución de la variable aleatoria a ordenar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cd094-7126-46f5-a957-b878b9b1971b",
   "metadata": {},
   "source": [
    "<a id='TeoremaDistribucionAsintoticaEstadisticaOrden'></a>\n",
    "### **Distribución Asintótica de una estadística de orden**\n",
    "Sean $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria con f. de distribución de $F_x$ y densidad $f_x$.\n",
    "Sea $p\\in(0,1)$ una probabilidad y $\\xi_p$ el (**único**) percentil asociado a dicha probabilidad. Si se tiene que \n",
    "\n",
    "$$ \n",
    "\\frac{k}{n} \\overset{}{\\underset{n\\to\\infty}{\\longrightarrow}} p\n",
    "$$\n",
    "\n",
    "entonces\n",
    "\n",
    "1) $X^{(k)} \\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \\xi_p$\n",
    "\n",
    "2) Además, si $f_x$ es continua alrededor de $\\xi_p$ y $f_x(\\xi_p)>0$, se tiene que :\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(X^{(k)}-\\xi_p) \\overset{d}{\\underset{n\\to\\infty}{\\longrightarrow}} N\\left( 0,\\frac{p(1-p)}{f_x^2(\\xi_p)} \\right)\n",
    "$$\n",
    "\n",
    "> Podemos ver que en algunos casos el percentil no es un valor único dado que en el siguiente ejemplo, el percentil asociado a la probabilidad $p$ tenemos que $\\xi_p$ toma un intervalo de valores. \n",
    "![](https://cdn.mathpix.com/snip/images/YInRGHONq1CnZ7XFWH-PPFVIMGxUNDu_1gyXxVh9POY.original.fullsize.png)\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Sea $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria normal, vamos a estudiar la distribución asintótica de la mediana $\\mu_e$.\n",
    "\n",
    "Recordemos que la mediana pretende estudiar la probabilidad de dejar a un 50% atras a toda la población. Como la normal es una densidad simétrica podemos decir que $\\xi_{0.5} = \\mu_e=\\mu$. \n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/-QiYRIRWlYfuDbIklPWrZAhBU6GSJLnWKU5o8oBm8qA.original.fullsize.png)\n",
    "\n",
    "Pero dada la función de distribución de una variable aleatoria tenemos tambien que $\\xi_{0.5}$ es único como podemos verificar en $F_X(\\xi_{0.5})=0.5$.\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/zVzz7dLmaEEhgnRMt0JFnANMWEGuuD_OH5R6PnznS50.original.fullsize.png)\n",
    "\n",
    "De este modo tenemos que \n",
    "\n",
    "$$\n",
    "\\mu_e \\overset{c.s.}{\\underset{n\\to\\infty}{\\longrightarrow}} \\mu=\\xi_{0.5}\n",
    "$$\n",
    "\n",
    "La segunda parte del teorema nos dice además que si se tiene que $f_X$ es continua alrrededor de $\\mu_e$ y además $f_X(\\mu_e) > 0$, lo cual es cierto entonces tenemos \n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\mu_e - \\mu) \\overset{d}{\\underset{n \\to \\infty}{\\longrightarrow}} N \\left( 0, \\frac{0.5 \\cdot 0.5}{f^2(\\mu)} \\right)\n",
    "$$\n",
    "\n",
    "Teniendo en cuenta que \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_X(\\mu)\n",
    "& = \\frac{1}{\\sqrt{2 \\pi} \\sigma}\\operatorname{exp}\\left[ \\frac{-(\\mu-\\mu)^2}{2\\sigma^2} \\right] \\\\\n",
    "& = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "luego $f_X^2(\\mu) = \\frac{1}{2 \\pi \\sigma^2}$. De este modo \n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\mu_e - \\mu) \\overset{d}{\\underset{n \\to \\infty}{\\longrightarrow}} N \\left( 0, \\frac{1}{4(\\frac{1}{2 \\pi \\sigma^2})} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\mu_e - \\mu) \\overset{d}{\\underset{n \\to \\infty}{\\longrightarrow}} N \\left( 0, \\frac{\\pi}{2}\\sigma^2 \\right)\n",
    "$$\n",
    "\n",
    "Dentro de los objetivos de este curso tambien podríamos decir, teniendo encuenta la delicadeza de la aproximación ([Teorema del límite central](#TeoremaLimiteCentral)) que \n",
    "\n",
    "$$\n",
    "\\mu_e \\approx N(\\mu , \\frac{\\pi\\sigma^2}{2n})\n",
    "$$\n",
    "\n",
    "> Podemos ver de forma muy informal que al tener funciones\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sqrt{n}(\\mu_e-\\mu) & \\approx N\\left(0,\\frac{\\pi\\sigma^2}{2}\\right) \\\\\n",
    "\\mu_e-\\mu & \\approx \\frac{1}{\\sqrt{n}}N\\left(0,\\frac{\\pi\\sigma^2}{2}\\right) \\\\\n",
    "\\mu_e-\\mu & \\approx \\frac{1}{\\sqrt{n}}N\\left(0\\frac{1}{\\sqrt{n}},(\\frac{1}{\\sqrt{n}})^2\\frac{\\pi\\sigma^2}{2}\\right) \\\\\n",
    "\\mu_e-\\mu & \\approx \\frac{1}{\\sqrt{n}}N\\left(0,\\frac{\\pi\\sigma^2}{2n}\\right) \\\\\n",
    "\\mu_e & \\approx \\frac{1}{\\sqrt{n}}N\\left(0,\\frac{\\pi\\sigma^2}{2n}\\right) + \\mu \\\\\n",
    "\\mu_e & \\approx \\frac{1}{\\sqrt{n}}N\\left(\\mu,\\frac{\\pi\\sigma^2}{2n}\\right) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b4836-3a0f-49d0-bdf3-32389342d024",
   "metadata": {},
   "source": [
    "## **Conceptos básicos y métodos de estimación**\n",
    "Recordemos que la inferencia estadística tiene como principal objetivo caracterizar a una población a partir de una muestra. Esta caracterización se hace determinando un modelo probabilístico a partir de la búsqueda de unos parámetros. Luego necesitamos que modelos caracterizan a la población y que parámetros siguen. Desde la inferencia estadística tenemos que necesitamos lo siguiente\n",
    "\n",
    "- Inferencia Estadística\n",
    "    - Estimación Puntual\n",
    "    - Estimación por Intervalo \n",
    "    - Prueba de Hipótesis\n",
    "    \n",
    "<a id='DefinicionParametro'></a>\n",
    "### **Parámetro**\n",
    "El parámetro es una **constante** $\\theta$ constitutiva del model probabilístico elegido para representa a la población.\n",
    "\n",
    "#### Ejemplo\n",
    "Sea $X$ una variable aleatoria tenemos que:\n",
    "\n",
    "- Si $X \\sim Exp(\\lambda)$ el valor de $\\theta=\\lambda$ es la constante constitutiva del modelo probabilístico que representa la población i.e. el parámetro del modelo probabilistico que representa la población.\n",
    "\n",
    "- Si $X \\sim N(\\mu,\\sigma)$ los valores de $\\theta_1=\\mu$ y de $\\theta_2=\\sigma$ son los parámetros del modelo probabilistico que representan a la población.\n",
    "\n",
    "- Si $X \\sim Ber(p)$ el valor $\\theta=p$ es el parámetro del modelo probabilistico que representa la población.\n",
    "\n",
    "<a id='DefinicionEspacioParametro'></a>\n",
    "### **Espacio del parámetro**\n",
    "Conjunto de valores de los parámetros que hace del modelo probabilístico un modelo válido. Se denotará por $\\Theta$\n",
    "\n",
    "#### Ejemplo\n",
    "Sea $X$ una variable aleatoria tenemos que:\n",
    "- Si $X \\sim Exp(\\lambda)$ el espacio $\\Theta \\subset \\mathbb{R}$  del parámetro $\\theta = \\lambda$ es $\\Theta=\\mathbb{R}^+=(0,\\infty)=\\{ \\lambda\\in\\mathbb{R} | \\ \\lambda > 0 \\}$ dado que \n",
    "\n",
    "$$\n",
    "f_X(x;\\theta) = \\theta e^{-\\theta x} I_{[0,\\infty)}(x)\n",
    "$$\n",
    "\n",
    "y para que sea un modelo válido de variables aleatorias continuas tenemos que tener que  $\\operatorname{Dom} f_x = \\mathbb{R}$, $f_x(x) \\geq 0 \\quad \\forall x \\in \\mathbb{R}$ y tambien $\\int_{\\mathbb{R}} f_x(x) dx = 1$. Tengamos en cuenta tambien que un modelo de variables aleatorias discretas sea valido necesitamos que $\\operatorname{Dom} P_x = \\mathbb{R}$, $P_X(x) \\geq 0$ y además $\\sum_{x \\in D_X} P_X(x) = 1$\n",
    "\n",
    "Luego $\\operatorname{Dom} f_X(x) = \\mathbb{R}$ dado que las funciones exponenciales están definidas para todo $\\mathbb{R}$ luego $\\theta \\in \\mathbb{R}$, pero $\\theta \\geq 0$ dado que es requisito que $f_X(x)= \\theta e^{-\\theta x} I_{[0,\\infty)}(x) \\geq 0$  y por el primer producto ($\\theta e^{}$) podriamos cambiar el signo de $f_X(x)$. Además es necesario que $\\theta > 0$ ya que el exponente sería 1 pero tiene un producto por un cero, entonces  $\\int_{0}^{\\infty} f_x(x) dx = 0$\n",
    "\n",
    "- Si $X \\sim N(\\mu,\\sigma)$  el espacio $\\Theta \\subset \\mathbb{R}^2$ de los parámetros es $\\Theta= \\{ (\\mu , \\sigma^2) \\in \\mathbb{R}^2  | \\ \\sigma \\in \\mathbb{R}^+ \\}$ ya que si pensamos en $f_X$ de la siguiente manera\n",
    "\n",
    "$$\n",
    "f_X(x;\\theta_1=\\mu,\\theta_2=\\sigma^2)=\\frac{1}{\\sqrt{2 \\pi} \\sigma}\\operatorname{exp}\\left[ \\frac{-(x-\\mu)^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "Vemos que $\\theta_2 = \\sigma^2$ debe ser tal que $\\theta_2 \\neq 0$ ya que tendriamos una división por cero. y para que sea no negativa entonces tenemos que $\\sqrt{\\theta_2}=\\sigma > 0$ ya que en caso contrario $\\frac{1}{\\sqrt{2 \\pi} \\sigma}$ sería negativo y al multiplicarlo por $\\operatorname{exp}\\left[ \\frac{-(x-\\mu)^2}{2\\sigma^2} \\right]$ tendriamos que la imagen $f_X(x)$ seria negativa. Al final la integral $\\int_{\\mathbb{R}} f_X(x) dx$ no pone otra condición ni sobre $\\theta_1$ ni sobre $\\theta_2$. \n",
    "\n",
    "- Si $X \\sim Bin(n,p)$ tenemos que $\\Theta= \\mathbb{N}\\times (0,1) = \\{(n,p)\\in\\mathbb{R} | \\ n \\in \\mathbb{N} \\land p\\in(0,1) \\}$ ya que recordemos\n",
    "\n",
    "$$\n",
    "P_X(x;\\theta_1=n,\\theta_2=p) = \\binom{n}{x}p^{x}(1-p)^{n-x} I_{0,1,2,\\cdots,n}(x)\n",
    "$$\n",
    "\n",
    "luego $\\operatorname{Dom}P_X = \\mathbb{R}$ pero tenemos que $\\theta_1=n \\in \\mathbb{N} \\cup \\{0\\}$ ya que $\\binom{n}{x}$, además tenemos que $\\theta_2=p \\in (0,1)$ ya que en caso contrario el producto $p^{x}(1-p)^{n-x}$ nos dejaría la imágen de $P_X$ negativa.\n",
    "\n",
    "<a id='DefinicionEstimar'></a>\n",
    "### **Estimar**\n",
    "Asignar un valor a los parámetros desconocidos del modelo probabilístico de tal manera que éste quede completamente **identificado**.\n",
    "\n",
    "*Esta definición de estimar realmente no contempla como se debería estimar de manera optima el parámetro o los parámetros*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0bbd8-3d7c-4e13-9adf-bbdaa552e8ee",
   "metadata": {},
   "source": [
    "<a id='DefinicionEstadistica'></a>\n",
    "### **Estadística**\n",
    "Dado un modelo probabilístico dependiente del parámetro $\\vec{\\theta} \\in \\mathbb{R}^q$ , y sea:\n",
    "\n",
    "$$\n",
    "\\mathbb{R}^n \\to \\mathbb{R}^m\n",
    "$$\n",
    "$$\n",
    "\\vec{t}:(X_{1},X_{2},\\cdots,X_{n}) \\to \\vec{t}(X_{1},X_{2},\\cdots,X_{m})\n",
    "$$\n",
    "\n",
    "Donde $X_{1},X_{2},\\cdots,X_{n}$ corresponde a una [(aleatoria*)](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#DefinicionMuestraAleatoria) del modelo especificado, $t$ es una **estadística** si no depende de ningún componente de $\\vec{\\theta}$ ni de alguna otra constante desconocida.\n",
    "\n",
    "<a id='DefinicionEstimadorPuntual'></a>\n",
    "### **Estimador puntual**\n",
    "Es una estadística de dimensión igual al número de componentes desconocidos de $\\vec{\\theta}$. Sus realizaciones se conocen como estimaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc76add-eb39-4e92-ab45-0bce4751bfa3",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "Sea $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria tal $X_1 \\sim N(\\mu,\\sigma^2)$ tenemos \n",
    "\n",
    "- $X_1$ és una estadística\n",
    "\n",
    "- $\\frac{1}{n} \\sum_{i=1}^n X_i = \\overline{X}_n$ és tambien una estadística\n",
    "\n",
    "- $\\frac{1}{n} \\sum_{i=1}^n X_i^2 $ és una estadística\n",
    "\n",
    "- $\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2 $ NO és una estadística\n",
    "\n",
    "- $\\sigma X_2$ NO és una estadística\n",
    "\n",
    "- $\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\overline{X})^2 = S_n^2$ és una estadística\n",
    "\n",
    "- $\\mu_e$ és una estadística\n",
    "\n",
    "> Es importante tener en cuenta que en la definición de estadística NO se restringe la posibilidad de que en la distribución de la estadística si dependa de los parámetros $\\vec{\\theta}$. Tambien la definición de estadística nos dice que ella esa una variable aleatoria y por tanto tiene las mismas propiedades de una variable aleatoria. Las realizaciones de la estadística de notan en letras mayúsculas y para el caso de estimador (estadística con la misma dimension que $\\vec{\\theta}$ parámetros desconocidos) tenemos la estimación (de los cuales dependen del azar y no se asugura ningun valor sobre ellos; el valor va a los estimadores). la idea de la estimación puntual es encontrar buenos estimadores de los parámetros del modelo probabilístico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf774c46-25f0-4f6a-8a0a-81d94a554de6",
   "metadata": {},
   "source": [
    "## **Construcción de Estimadores**\n",
    "\n",
    "<a id='DefinicionMetodoAnalogia'></a>\n",
    "### **Método de analogía**\n",
    "Designar como estimador a una estadística que desempeñe el mismo rol que el parámetro en la población.\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Dada una muestra aleatoria $X_{1},X_{2},\\cdots,X_{n}$ tal que $X_{1} \\sim P(\\lambda)$, encontrar uno o varios estimadores por analogía para el parámetro $\\lambda$. \n",
    "\n",
    "Tenemos que la función de distribución para una variable aleatoria $X$ tal que $X \\sim P(\\lambda)$ es \n",
    "\n",
    "$$\n",
    "p_x(x;\\lambda) = e^{-\\lambda}\\frac{\\lambda^x}{x!} I_{\\{ 0,1,2,\\cdots \\}}(x)\n",
    "$$\n",
    "\n",
    "Veamos que el parámetro $\\lambda = \\theta$ tiene como espacio $\\Theta = \\{ \\lambda \\in \\mathbb{R} | \\ \\lambda > 0 \\}$ dado que: \n",
    "\n",
    "- $\\operatorname{Dom} f_x = \\mathbb{R}-\\{0\\}$ puesto que $e^{-\\lambda}$ está definida para todos los reales pero $\\frac{\\lambda^x}{x!}$ no está definida en cero ya que $\\lambda^x$ podria dar una indeterminación de tipo $0^0$\n",
    "\n",
    "- $f_X(x) \\geq 0$ y a pesar de que $e^{-\\lambda}$ es positiva para todo $\\mathbb{R}$ tenemos que $\\frac{\\lambda^x}{x!}$ no lo sería para los exponentes $x$ impares.\n",
    "\n",
    "- $\\sum_{x=0}^\\infty p_x(x;\\lambda) = \\sum_{x=0}^\\infty e^{-\\lambda}\\frac{\\lambda^x}{x!} = e^{-\\lambda} \\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} = e^{-\\lambda}e^{\\lambda} = 1$ luego no tenemos más restricciones adicionales para $\\lambda$.\n",
    "\n",
    "Dado que, para la variable aleatoria, de nuestra muestra aleatoria, $X_1 \\sim P(\\lambda)$ tenemos $E(X_1) = \\lambda = \\operatorname{Var}(X_1)$ (existen). Teniendo en cuenta ahora las [propiedades de la media muestra](#TeoremaPropiedadesMediaMuestral) tenemos que $E(\\overline{X}_n) = \\lambda$. De este modo el mejor estimador por analogía de $\\lambda$ es $\\hat{\\lambda}_1 = \\overline{X}_n$ [media muestral](#DefinicionMediaMuestral) (*el cual no depende de nuestro parámetro*). Tambien $\\hat{\\lambda}_2 = S_n^2$ es otro estimador por analogía ya que podemos ver en la [definición de varianza muestral](#DefinicionVarianzaMuestral) que no depende del parámetro y que ademas por [propiedades de la varianza muestral](#TeoremaPropiedadesVarianzaMuestral) tenemos que $E(S_n^2)=\\lambda$.\n",
    "\n",
    "> *Es importante tener en cuenta que no podemos llegar a decir que $\\lambda=\\overline{X}_n$ dado que estariamos suponiendo que somos alguna deidad que de alguna manera encontro la forma de calcular el parametro sobre la población. Lo correcto es decir que para el parametro $\\lambda$ de la poblacipon tengo el estimador $\\hat{\\lambda}$ el cual dadas la propiedades vistas anteriormente sabemos que esta cerca del valor exacto del parámetro. De este modo $\\hat{\\lambda}_{est} = \\overline{x}_n$ seria meramente una estimación del parametro $\\lambda$*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0e4b8-42f8-4537-903e-c1914daa62c9",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "Dada una muestra aleatoria $X_{1},X_{2},\\cdots,X_{n}$ tal que $X_{1} \\sim N(\\mu,\\sigma^2)$, encontrar uno o varios estimadores por analogía para el parámetro $\\mu$ y para $\\sigma^2$.\n",
    "\n",
    "En los ejemplos de [espacio del parámetro](#DefinicionEspacioParametro) vimos que en efecto $\\mu \\in \\mathbb{R}$ y $\\sigma \\in \\mathbb{R}^+$. ahora tenemos que ver que para \n",
    "\n",
    "$$\n",
    "f_X(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2 \\pi} \\sigma}\\operatorname{exp}\\left[ \\frac{-(x-\\mu)^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "Cuales son los mejores estimadores de $\\mu$ y de $\\sigma$. Como tenemos que para la población $E[X]=E[\\overline{X}_n] = \\mu$ tenemos que primero que $\\hat{\\mu}_1=\\overline{X}_n$. \n",
    "Segundo vimos tambien en el ejemplo de [distribuciones asintóticas de estadísticas de orden](#TeoremaDistribucionAsintoticaEstadisticaOrden) que para funciones para una una muestra normal, dada su simetría tenemos que $\\mu_{\\text{mediana}} = \\xi_{0.5} = \\mu$ por lo que podriamos deja $\\hat{\\mu}_2 = \\mu_e$ ([mediana muestral](#DefinicionMedianaMuestral)). La moda definido como el máximo de la función de distribución, en la distribución normal, se puede verificar que, $\\mu_{\\text{moda}} = \\mu$; así $\\hat{\\mu}_3 = X_o$ (*TO-DO notacion moda muestral*)\n",
    "\n",
    "[Representación gráfica media, mediana, moda](https://upload.wikimedia.org/wikipedia/commons/3/33/Visualisation_mode_median_mean.svg)\n",
    "\n",
    "Para el parámetro $\\sigma^2$ de momento conformemonos con $\\hat{\\sigma}^2 = S_n^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc95277-a201-4ec9-ac40-4768d8f75a02",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "Dada una muestra aleatoria $X_{1},X_{2},\\cdots,X_{n}$ tal que $X_{1} \\sim Ber(p)$, encontrar uno o varios estimadores por analogía para el parámetro $p$. Tenemos que \n",
    "\n",
    "$$\n",
    "p_X(k;p)=\n",
    "\\begin{cases} \n",
    "    p & \\text{si } k=1, \\\\ \n",
    "    q=1-p & \\text{si } k=0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Luego tenemos que el espacio del parámetro $\\Theta = \\{ p \\in \\mathbb{R} | \\ 0 \\leq p \\leq 1\\}$ ya que \n",
    "\n",
    "- $\\operatorname{Dom}p_X = \\mathbb{R}$ luego $p \\in \\mathbb{R}$; aunque\n",
    "\n",
    "- $p_X(k) \\leq 0 $ luego $p \\geq 0$ por la primera condición y por la segunda es necesario que tambien $q \\geq 0$ por lo que $0 \\leq p \\leq 1$. Esto se sostiene ya que \n",
    "\n",
    "- $\\sum_{k=0}^1 p_X(k) = p + q = 1$\n",
    "\n",
    "Como en la población $E[X_i]=p$ y tenemos que $E[\\overline{X}_n]=p$ tenemos que el mejor estimado por analogía es $\\hat{p} = \\overline{X}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c5e87-87b6-468d-981d-9cad3e44f2be",
   "metadata": {},
   "source": [
    "> *Este metodo no es del todo infalible dado que supongamos que tenemos una distribución como $X \\sim Beta(\\alpha,\\beta)$ donde tenemos que en la población tendríamos $E(X_i)=\\frac{\\alpha}{\\alpha-\\beta}$ y si miramos la varianza tambien tendremos algun resultado no tan eficaz y en muchas otras ocasiones no es posible establecer la analogía. Tambien en modelos muy complejos el estimador $\\hat{\\theta}$ podria tomar valores fuera de $\\Theta$ espacio de la muestra. Pero más sin embargo es un método sencillo y por esto es que vale la pena mostrarlo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f647723-5e34-4992-90bf-51e6d52d9966",
   "metadata": {},
   "source": [
    "<a id='DefinicionMetodoMomentos'></a>\n",
    "### **Método de los momentos**\n",
    "Suponga que $\\vec{\\theta} \\in \\mathbb{R}^q$ y \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j(\\vec{\\theta}) & = E_{\\vec{\\theta}} (X^j); \\quad 1 \\leq j \\leq q \\\\\n",
    "\\overset{\\sim}{\\mu}_j & = \\frac{1}{n} \\sum_{i=1}^n X_i^j; \\quad 1 \\leq j \\leq q \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Adicionalmente, suponga que existe una función 1-1 tal que:\n",
    "\n",
    "$$\n",
    "\\mathbb{R}^q \\to \\mathbb{q}\n",
    "$$\n",
    "$$\n",
    "f: \\vec{\\theta} \\to f(\\vec{\\theta}) = (\\mu_1(\\vec{\\theta}),\\mu_2(\\vec{\\theta}),\\cdots,\\mu_q(\\vec{\\theta}))\n",
    "$$\n",
    "\n",
    "Los estimadores por el método de los momentos son las soluciones ($\\vec{\\hat{\\theta}}$) al sistema de ecuaciones:\n",
    "\n",
    "$$\n",
    "\\overset{\\sim}{\\mu}_j = \\mu_j(\\vec{\\hat{\\theta}}); \\quad 1 \\leq j \\leq q\n",
    "$$\n",
    "\n",
    "> *En síntesis el método de los momentos lo que nos dice es que se tienen $q$ parámetros desconocidos, y considerando los momentos $j$-ésimos ordinarios (o momentos poblacionales) de la variable aleatoria $E_{\\vec{\\theta}}(X_1^j) = \\mu_j$. Entonces vamos a encontrar de $1$ hasta el $q$-ésimo momento muestral $\\overset{\\sim}{\\mu}_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j$. Fijemonos entonces que al final $\\mu_j$ va a depender de los $\\vec{\\theta}$ parámetros del modelo. Pero en principio $\\overset{\\sim}{\\mu}_j$ no va a depender de estos parámetros y por lo tanto es una estadística. Luego se nos pide suponer que para cada vector de parámetros $\\vec{\\theta}$ se tenga una determinanción 1-1 de los momentos ordinarios poblacionales i.e. que dos momentos poblacionales $\\mu_i(\\theta)_a$ y $\\mu_i(\\theta)_b$ son tales que si $\\mu_i(\\theta)_a = \\mu_i(\\theta)_b$ para todo $0 \\leq i \\leq q$ entonces $\\vec{\\theta}_a = \\vec{\\theta}_b$. Si todo esto se cumple entonces, las nuevas soluciones $\\vec{\\hat{\\theta}}$ al sistema $\\overset{\\sim}{\\mu}_j = \\mu_j(\\vec{\\hat{\\theta}})$, seran los estimadores de los parámetros de nuestro modelo.*\n",
    "\n",
    "> En general no tenemos siempre que el método de los momentos nos deja un sistema de ecuaciones lineales y por lo tanto a menudo tampoco va a ser el mejor camino de estimación puntual de los parámetros del modelo probabilístico a seguir.\n",
    "\n",
    "> <span style='color:lime'>**(+)**</span> Viene del hecho que $\\overset{\\sim}{\\mu}_r = \\frac{1}{n} \\sum_{i=1}^n X_i^r \\overset{p}{\\underset{n \\to \\infty}{\\longrightarrow}} E[X_i^r] = f(\\vec{\\theta})$ 1a1.\n",
    "\n",
    "> <span style='color:red'>**(-)**</span> Puede dar soluciones que no están en el espacio del parámetro\n",
    "\n",
    "> <span style='color:red'>**(-)**</span> El sistema $q \\times q$ puede ser no lineal y difícil de solucionar (inestabilidad).\n",
    "\n",
    "> <span style='color:red'>**(-)**</span> Puede no tener solución analítica.\n",
    "\n",
    "> <span style='color:red'>**(-)**</span> Deben existir los momentos dela población.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2c0b3-d84c-43aa-995f-7152d2c51a58",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "Sea $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria tal que $N(\\mu,\\sigma^2)$. Determine un estimador por el método de los momentos para el parámetro $\\vec{\\theta} = (\\mu , \\sigma^2)$\n",
    "\n",
    "Sea entonces \n",
    "\n",
    "$$\n",
    "\\mu_1(\\vec{\\theta}) = E_{\\vec{\\theta}}(X_i^1) = E_{\\vec{\\theta}}(X_i) = \\mu \\quad \n",
    "\\text{ y para } \\quad \n",
    "\\overset{\\sim}{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n X_i^1 = \\frac{1}{n} \\sum_{i=1}^n X_i = \\overline{X}_n\n",
    "$$\n",
    "\n",
    "además dado que $\\operatorname{Var}(X)=E(X^2)-[E(X)]^2$ cuando $E(X) < \\infty$ entonces $E(X^2) = [E(X)]^2+\\operatorname{Var}(X)$ y así\n",
    "\n",
    "$$\n",
    "\\mu_2(\\vec{\\theta}) = E_{\\vec{\\theta}}(X_i^2) = \\mu^2 + \\sigma^2 \\quad \n",
    "\\text{ y para } \n",
    "\\quad \\overset{\\sim}{\\mu}_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \n",
    "$$\n",
    "\n",
    "De este modo entonces los estimadores por el método de los momentos de $\\mu$ y $\\sigma^2$ ( notados como $\\hat{\\mu}$ y  $\\hat{\\sigma}^2$ ) son las soluciones al siguiente sistema $2\\times2$ de ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mu_1 (\\hat{\\mu},\\hat{\\sigma}^2) & = \\overset{\\sim}{\\mu}_1 \\\\\n",
    "    \\mu_2 (\\hat{\\mu},\\hat{\\sigma}^2) & = \\overset{\\sim}{\\mu}_2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "luego \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mu_1 (\\hat{\\mu},\\hat{\\sigma}^2) & = \\overline{X}_n \\\\\n",
    "    \\mu_2 (\\hat{\\mu},\\hat{\\sigma}^2) & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "como tenemos que $\\mu_1(\\vec{\\theta}) = \\mu$ y $\\mu_2(\\vec{\\theta}) = \\mu^2 + \\sigma^2$ entonces despejando a la izquierda tenemos\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{\\mu} & = \\overline{X}_n \\\\\n",
    "    \\hat{\\mu}^2 + \\hat{\\sigma}^2 & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "entonces para el momento dos tenemos que $\\hat{\\sigma}^2$ es\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\overline{X}_n^2 + \\hat{\\sigma}^2 & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\\\\n",
    "    \\hat{\\sigma}^2 & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\overline{X}_n^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Podriamos decir que hemos llegado al resultado, pero al tener una resta para la estimación de $\\sigma^2$ y sabiendo que $\\sigma > 0$, ¿es posible que el estimador $\\hat{\\sigma}^2 < 0$ en alguna estimación? pero se puede probar que $\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\overline{X}_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i-\\overline{X})^2$ lo cual nos quita la preocupación.\n",
    "\n",
    "> Notemos que además no obtuvimos $S_n^2$ por un factor de $\\frac{n}{n-1} > 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb5afe-b976-4975-b226-bb8064da8295",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "Sea $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria donde la distribución de $X_1 \\sim \\Gamma(\\alpha,\\beta)$. Para determinar un estimador por momentos del parámetro $\\vec{\\theta}=(\\alpha,\\beta)$ veamos primero que \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x;\\alpha ,\\beta )\n",
    "&={\\frac {\\beta ^{\\alpha }x^{\\alpha -1}e^{-\\beta x}}{\\Gamma (\\alpha )}}\n",
    "\\quad {\\text{ para }}x>0\\quad \\alpha ,\\beta >0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Sea entonces \n",
    "\n",
    "$$\n",
    "\\mu_1(\\vec{\\theta}) = E_{\\vec{\\theta}}(X_i^1) = E_{\\vec{\\theta}}(X_i) = \\frac{\\alpha}{\\beta} \\quad \n",
    "\\text{ entonces } \\quad \n",
    "\\overset{\\sim}{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n X_i^1 = \\frac{1}{n} \\sum_{i=1}^n X_i = \\overline{X}_n\n",
    "$$\n",
    "\n",
    "además dado que $\\operatorname{Var}(X)=E(X^2)-[E(X)]^2$ cuando $E(X) < \\infty$ entonces $E(X^2) = [E(X)^2]+\\operatorname{Var}(X)$ , así\n",
    "\n",
    "$$\n",
    "\\mu_2(\\vec{\\theta}) = E_{\\vec{\\theta}}(X_i^2) = \\left( \\frac{\\alpha}{\\beta} \\right)^2 + \\frac{\\alpha}{\\beta^2} \\quad \n",
    "\\text{ entonces } \n",
    "\\quad \\overset{\\sim}{\\mu}_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \n",
    "$$\n",
    "\n",
    "De este modo entonces los estimadores por el método de los momentos de $\\alpha$ y $\\beta$ ( notados como $\\hat{\\alpha}$ y  $\\hat{\\beta}$ ) son las soluciones al siguiente sistema $2\\times2$ de ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mu_1 (\\hat{\\alpha},\\hat{\\beta}) & = \\overset{\\sim}{\\mu}_1 \\\\\n",
    "    \\mu_2 (\\hat{\\alpha},\\hat{\\beta}) & = \\overset{\\sim}{\\mu}_2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\hat{\\alpha}}{\\hat{\\beta}} & =\\overline{X}_n \\\\\n",
    "    \\left( \\frac{\\hat{\\alpha}}{\\hat{\\beta}} \\right)^2 + \\frac{\\hat{\\alpha}}{\\hat{\\beta}^2} & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> En este punto es importante tener en cuenta que ni en el ejemplo anterior, ni en este, vamos a verificar directamente que la función $f$ que necesitamos sea 1a1; y esto es porque a lo que queremos llegar es al sistema de ecuaciones en donde si tienemos una única solucion, entonces sabremos que existe la función.\n",
    "\n",
    "Fijemonos por un momento que \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\hat{\\alpha}}{\\hat{\\beta}} & =\\overline{X}_n \\\\\n",
    "    \\left( \\frac{\\hat{\\alpha}}{\\hat{\\beta}} \\right)^2 + \\frac{\\hat{\\alpha}}{\\hat{\\beta}}\\frac{1}{\\hat{\\beta}} & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde remplazando la primera ecuación por la segunda tenemos\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\hat{\\alpha}}{\\hat{\\beta}} & =\\overline{X}_n \\\\\n",
    "    \\left( \\overline{X}_n \\right)^2 + \\overline{X}_n\\frac{1}{\\hat{\\beta}} & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "y de este modo tenemos que \n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta}\n",
    "& = \\frac{\\overline{X}_n}{\\frac{1}{n} \\sum_{i=1}^n X_i^2 -  \\left( \\overline{X}_n \\right)^2 }\\\\\n",
    "& = \\frac{\\overline{X}_n}{\\frac{1}{n} \\sum_{i=1}^n \\left( X_i - \\overline{X}_n \\right)^2 }\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "y como por la primera ecuación teniamos que $\\hat{\\alpha} = \\hat{\\beta}\\overline{X}_n$ entonces\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} =  \\frac{\\overline{X}_n^2}{\\frac{1}{n} \\sum_{i=1}^n \\left( X_i - \\overline{X}_n \\right)^2 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f661c8-fa20-46fe-b4de-aeafdfe13e5c",
   "metadata": {},
   "source": [
    "Vamos a ver ahora un ejemplo en donde no es necesario tomar el primer y segundo momento de la variable aleatoria, puede ser el segundo y el tercero si se tienen dos parámetros(si los tiene) o alguna combinación a conveniencia dependiendo de los parámetros y de la naturaleza de la distribución.\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Sea $X_{1},X_{2},\\cdots,X_{n}$ una muestra aleatoria donde $X_1 \\sim U(-\\theta,\\theta)$. Determine un estimador por momentos del parámetro donde \n",
    "\n",
    "$$\n",
    "f_X(x;\\theta) = \\frac{1}{2\\theta} I_{(.\\theta,\\theta)}(x)\n",
    "$$\n",
    "\n",
    "Tenemos que el espacio del vector de parámetros es $\\Theta=\\mathbb{R}^+$. Para encontrar un estimador por el método de parámetros veamos por un momento que\n",
    "\n",
    "$$\n",
    "\\mu_1(\\theta) = E_{\\vec{\\theta}}(X_i^1) = E_{\\vec{\\theta}}(X_i) = \\frac{\\theta+(-\\theta)}{2} = 0 \\quad \n",
    "\\text{ entonces } \\quad \n",
    "\\overset{\\sim}{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n X_i^1 = \\frac{1}{n} \\sum_{i=1}^n X_i = \\overline{X}_n\n",
    "$$\n",
    "\n",
    "y de este modo tenemos que igualar pensando que $\\overline{X}_n = 0$ donde no tenemos ninguna información de $\\hat{\\theta}$ nuestro estimador a entontrar. Esto no nos sirve, por lo tanto veamos mejor que está sucediendo en $\\mu_2(\\theta)$\n",
    "\n",
    "$$\n",
    "\\mu_2(\\vec{\\theta}) = E_{\\vec{\\theta}}(X_i^2) = \\operatorname{Var}(X) + [E(X)]^2 = \\operatorname{Var}(X) = \\frac{(\\theta-(-\\theta))^2}{12} = \\frac{(2\\theta)^2}{12} = \\frac{\\theta^2}{3}\\quad \n",
    "\\text{ entonces } \n",
    "\\quad \\overset{\\sim}{\\mu}_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \n",
    "$$\n",
    "\n",
    "nos deja que \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_2 (\\hat{\\theta}) & = \\overset{\\sim}{\\mu}_2  \\\\\n",
    "\\frac{\\hat{\\theta^2}}{3} & = \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\\\\n",
    "\\hat{\\theta} & = \\sqrt{ \\frac{3}{n} \\sum_{i=1}^n X_i^2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> En la distribución de Cauchy no podemos  encontrar ningún momento, por lo tanto el método de los moementos no se podría aplicar siquiera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c629f-225d-4853-b2b1-0952fa72abb8",
   "metadata": {},
   "source": [
    "<a id='DefinciónMetodoOptimizacion'></a>\n",
    "### **Método de optimización**\n",
    "\n",
    "#### Repaso conceptos de optimización del cálculo\n",
    "\n",
    "##### **Definición de partes de un conjunto**\n",
    "Sea $U$ un conjunto tenemos que \n",
    "- $Int(U) \\subseteq U$ es tal que para todo $x \\in Int(U)$ existe una bola $B_{\\varepsilon}(x)$ (de radio $\\varepsilon > 0$ y centro en $x$) tal que $B_{\\varepsilon}(x) \\subset Int(U)$\n",
    "\n",
    "- $Borde(U) \\subseteq U$ es tal que $Borde(U) = U - Int (U)$\n",
    "\n",
    "- $Frontera(U)$ o $\\partial U$ a diferencia del borde, son los puntos límites de $U$\n",
    "\n",
    "###### Ejemplo\n",
    "- Sea $U=\\mathbb{R}$ entonces $\\mathbb{R}=Int(\\mathbb{R})$, $Borde(\\mathbb{R})=\\emptyset$ y $Frontera(\\mathbb{R})=\\{ -\\infty,\\infty \\}$\n",
    "\n",
    "- Sea $U=\\mathbb{N}$ entonces $\\emptyset=Int(\\mathbb{N})$, $Borde(\\mathbb{N})=\\mathbb{N}$ y $Frontera(\\mathbb{R})=\\{ -\\infty,\\infty \\}$\n",
    "\n",
    "##### **Definicion de problema de optimización**\n",
    "Sea $f:U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ con $U \\neq \\emptyset$. Un problema de optimización se puede representar como\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{x} \\in U}{\\operatorname{min}} f(\\mathbf{x}) \\quad \\text{ y } \\quad\n",
    "\\underset{\\mathbf{x} \\in U}{\\operatorname{max}} f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "donde por un lado nos preguntamos por la imagen f(\\mathbf{x}) menor a todas las otras dentro de $U$ y por el otro a la imagen $f(\\mathbf{x})$ mayor a todas las otras dentro de $U$.\n",
    "\n",
    "En caso de que el problema tenga solución, ésta se puede escribir como:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x} \\in U}{\\operatorname{arg min}} f(\\mathbf{x}) \\quad \\text{ y } \\quad\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x} \\in U}{\\operatorname{arg max}} f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "- Esta breve introducción busca usar resultados de optimización en conjunto abiertos, por tal razón, se pide que el interior sea no vacío. No se invluyen es este caso resultados de optimización entera o combinatoria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3806da-0d4a-449f-af5d-2d851df3ab14",
   "metadata": {},
   "source": [
    "##### **Definicion de optimo global**\n",
    "Sea $f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$. si tenemos que\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}^*) \\leq f(\\mathbf{x}) \\ , \\ \\forall \\mathbf{x} \\in U\n",
    "$$\n",
    "\n",
    "entonces $\\mathbf{x}^*$ es un mínimo global de $f$. Si por otro lado tenemos que\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}^*) \\geq f(\\mathbf{x}) \\ , \\ \\forall \\mathbf{x} \\in U\n",
    "$$\n",
    "\n",
    "entonces $\\mathbf{x}^*$ es un máximo global de $f$.\n",
    "\n",
    "##### **Definicion de optimo local**\n",
    "\n",
    "Sea $f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$. si tenemos que\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}^*) \\leq f(\\mathbf{x}) \\ , \\ \\forall \\lVert \\mathbf{x}-\\mathbf{x}^* \\rVert < \\varepsilon\n",
    "$$\n",
    "\n",
    "entonces $\\mathbf{x}^*$ es un mínimo local de $f$. Si por otro lado tenemos que\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}^*) \\geq f(\\mathbf{x}) \\ , \\ \\forall \\lVert \\mathbf{x}-\\mathbf{x}^* \\rVert < \\varepsilon\n",
    "$$\n",
    "\n",
    "entonces $\\mathbf{x}^*$ es un máximo local de $f$.\n",
    "\n",
    "##### **Definicion de punto estacionario**\n",
    "\n",
    "Sea $f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ diferenciable. si tenemos que\n",
    "\n",
    "$$\n",
    "\\nabla f(\\hat{\\mathbf{x}}) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "entonces $\\hat{\\mathbf{x}}$ es un punto estacionario de $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2573d75-8547-4bde-8ad4-6448a0a12ed5",
   "metadata": {},
   "source": [
    "La estrategia para resolver un problema de optimización, asumiendo a $f$ diferenciable és\n",
    "\n",
    "<a id='Def.EstrategiÓptimosLocalesGlobales'></a>\n",
    "![](https://cdn.mathpix.com/snip/images/u4ZTHK9oQeodOmNgDjY8s3RIgScrYchnR1iiIUIIRi0.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f701a-8f49-429c-aeae-d68d4d648aed",
   "metadata": {},
   "source": [
    "##### **Condiciones del primer y segundo orden ótimos locales (con $U$ abierto)**\n",
    "\n",
    "###### **Condición necesaria de primer orden**\n",
    "\n",
    "Sea $f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ diferenciable y $\\mathbf{x}^*$ un mínimo/máximo local de $f$, entonces\n",
    "\n",
    "$$\n",
    "\\nabla f (\\mathbf{x}^*) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "###### **Condición necesaria de segundo orden**\n",
    "\n",
    "Sea $f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ dos veces diferenciable y si $\\mathbf{x}^*$ un mínimo local de $f$ entonces\n",
    "\n",
    "$$\n",
    "\\nabla f (\\mathbf{x}^*) = \\mathbf{0}\n",
    "$$\n",
    "y además la matriz\n",
    "$$\n",
    "\\nabla^2 f (\\mathbf{x}^*) \n",
    "$$\n",
    "es semidefinida positiva. \n",
    "\n",
    "Si $\\mathbf{x}^*$ un máximo local de $f$ entonces\n",
    "\n",
    "$$\n",
    "\\nabla f (\\mathbf{x}^*) = \\mathbf{0}\n",
    "$$\n",
    "y además la matriz\n",
    "$$\n",
    "\\nabla^2 f (\\mathbf{x}^*) \n",
    "$$\n",
    "es semidefinida negativa.\n",
    "\n",
    "###### **Condición suficiente de segundo orden**\n",
    "Sea $f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ dos veces diferenciable y $ \\mathbf{x}^*$ es tal que $\\nabla f(\\mathbf{x}^*) = \\mathbf{0}$ y si además \n",
    "\n",
    "$$\n",
    "\\nabla^2 f (\\mathbf{x}^*) \n",
    "$$\n",
    "\n",
    "es positiva definida entonces $\\mathbf{x}^*$ es un mínimo local de $f$ . Si \n",
    "\n",
    "$$\n",
    "\\nabla^2 f (\\mathbf{x}^*) \n",
    "$$\n",
    "\n",
    "es negativa definida entonces $\\mathbf{x}^*$ es un máximo local de $f$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bded776-18f1-427e-8057-78dd985f9537",
   "metadata": {},
   "source": [
    "###### **Condiciones de segundo orden**\n",
    "\n",
    "Sea $f(x,y):U \\subset \\mathbb{R}^2 \\to \\mathbb{R}$ diferenciable dos veces. Para clasificar los puntos estacionarios, se calcula la matriz Hessiana:\n",
    "\n",
    "$$\n",
    "\\nabla^{2} f(x, y)=\\left(\\begin{array}{ll}\n",
    "\\frac{\\partial^{2} f}{\\partial x^{2}}(x, y) & \\frac{\\partial^{2} f}{\\partial y \\partial x}(x, y) \\\\\n",
    "\\frac{\\partial^{2} f}{\\partial y \\partial x}(x, y) & \\frac{\\partial^{2} f}{\\partial y^{2}}(x, y)\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "para toda $(x,y) \\in Int(U)$.\n",
    "\n",
    "1) Si $|\\nabla^{2} f(x, y)| >0$ (*determinante mayor a cero*), el punto $(\\hat{x},\\hat{y})$ es un mínimo o máximo local.\n",
    "\n",
    "    1.1) Si $\\frac{\\partial^2 f}{\\partial x^2} (\\hat{x},\\hat{y})$ y $\\frac{\\partial^2 f}{\\partial y^2} (\\hat{x},\\hat{y})$ son positivos, es un mínimo.\n",
    "    \n",
    "    1.2) Si $\\frac{\\partial^2 f}{\\partial x^2} (\\hat{x},\\hat{y})$ y $\\frac{\\partial^2 f}{\\partial y^2} (\\hat{x},\\hat{y})$ son negativos, es un máximo.\n",
    "    \n",
    "2) Si $|\\nabla^{2} f(x, y)| < 0$, el punto es de silla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765be7f1-e993-44fe-8e42-e5b43d570738",
   "metadata": {},
   "source": [
    "##### **En busca de un óptimo global (con U abierto)**\n",
    "- Dependiendo de la naturaleza de $U$ y del comportamiento de $f$, un óptimo local puede corresponder con un óptimo global.\n",
    "- Algunos recursos que pueden ayudar a justificar la existencia y correspondencia de los óptimos globales son los siguientes:\n",
    "\n",
    "###### **Función Convexa/Cóncava**\n",
    "Sea $f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ dos veces diferenciable. $f$ es (**estrictamente**) convexa/*concava* si $\\nabla^2 f (\\mathbf{x})$ es positiva/*negativa* (**definida**) semidefinida, para toda $\\mathbf{x} \\in U$\n",
    "\n",
    "###### **Condición suficiernte de segundo orden (convexidad/concavidad)**\n",
    "Sea $f:U \\subset \\mathbb{R}^n \\to \\mathbb{R}$\n",
    "1) Si $f$ es convexa/*cóncava* , todo mínimo/*máximo* local es global.\n",
    "2) Si $f$ es estrictamente convexa/*cóncava* y tiene un mínimo/*máximo* local, este es único global.\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/Bek-87FVzfBC67cJbgIf8wdDZrP82i61eCqEdjGzmdU.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6836300-0832-4963-97da-ec2445b914d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### **Teorema de unimodalidad (Mixed models, Demidenko)**\n",
    "Sea $f:\\mathbb{R} \\to \\mathbb{R}$ dos veces deferenciable y tal que cuando $\\lVert \\mathbf{x} \\rVert \\to \\infty$ , $f \\to \\infty (\\infty)$. Si en cada punto estacionario, la matriz Hessiana es positiva / *negativa* definida; entonces $f$ tiene un único mínimo/*máximo* en $\\mathbb{R}^n$.\n",
    "\n",
    "###### **Condición suficiente de primer orden en una variable.**\n",
    "Sea $f:\\mathbb{R} \\to \\mathbb{R}$ diferenciable. Si $\\frac{df}{dx}(x^*)=0$, $\\frac{df}{dx}(x) > (<) 0$ para $x < x^*$ y $\\frac{df}{dx}(x) < (>) 0$ para $x>x^*$ es un máximo/*minimo* global de $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de870db-8aa6-4cea-94dd-80da221b071b",
   "metadata": {},
   "source": [
    "##### Ejemplo\n",
    "Sea $f(x) = x + \\frac{1}{x} \\ , \\quad x\\in \\mathbb{R}-\\{0\\}$. Estudie la existencia de extremos locales y globales. \n",
    "\n",
    "Teniendo en cuenta la [estrategia vista anteriormente](#Def.EstrategiÓptimosLocalesGlobales) vemos que  $U=\\mathbb{R}-\\{0\\}= int(U)$. Sabiendo eso $\\frac{d}{dx} =1 + \\frac{-1}{x^2}$ y $\\frac{d^2}{dx^2} =\\frac{2}{x^3}$. De este modo tenemos que $1+\\frac{-1}{x^2}=0$ siempre que $x_1^* = 1$ y $x_2^*=-1$. y como $\\frac{d^2}{dx^2} =\\frac{2}{x^3} < 0$ cuando $x<0$ (cóncava) y $\\frac{d^2}{dx^2} =\\frac{2}{x^3} > 0$ cuando $x>0$(convexa). Tenemos que $x_1^*$ es un mínimo local dado que $f(x_1^*)>f(x_2^*)$ y $x_2^*$ es un máximo local por el mísmo hecho anterior.\n",
    "\n",
    "Ya habiendo visto que sucede en el $int(U)=U$ tenemos que en el borde $U-int(U)=\\emptyset$ no tenemos puntos que mirar.\n",
    "\n",
    "Para saber que sucede en las fronteras $\\partial U = \\{-\\infty,0,\\infty \\}$ veamos que pasa en los valores más cercanos a cero dado que hacia el más infinito o el menos infinito la función se va igual hacia el infinito. De igual modo $\\lim_{x \\to 0^-} (1+\\frac{1}{x}) = -\\infty$ y $\\lim_{x \\to 0^+} (1+\\frac{1}{x}) = \\infty$ tenemos que $x_1^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5a0ad-6ce6-468a-859f-17d06c6c9e57",
   "metadata": {},
   "source": [
    "##### Ejemplo\n",
    "Sea $f(x,y)=(x-4)^2+(y-2)^2$. Estudie la existencia de extremos locales y globales.\n",
    "\n",
    "Para el dominio de $U$ el cual es $\\operatorname{Dom} f = \\mathbb{R}^{2}$ tenemos que $int (U)=U$. Luego como $f$ es diferenciable, para ver $\\nabla f = [ \\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial x}]$ es tal que $\\nabla f(\\mathbf{x}^*) = [ \\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial x}] = [0,0] = \\mathbf{0}$ realizamos $\\frac{\\partial}{\\partial x}((x-4)^2+(y-2)^2)=2(x-4)$ y además $\\frac{\\partial}{\\partial x}((x-4)^2+(y-2)^2)=2(y-2)$ luego tenemos que el punto solución para el cual $2(x^*-4)=0$ y $2(y^*-2)=0$ es $x^*=4$ y $y^*=2$.\n",
    "\n",
    "Por lo tanto realizando la matriz Hessiana tenemos que \n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial y \\partial x} \\\\\n",
    "\\frac{\\partial^2}{\\partial x \\partial y} & \\frac{\\partial^2}{\\partial y^2}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "La cual podemos verificar que es definida positiva y de este modo tenemos que $f$ es convéxa y como esto es por todo el dominio de $f$ tenemos que ella es estrictamente convéxa y por lo tanto $f$ es un mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a78e2-d133-4e53-9c0f-06e0b2389191",
   "metadata": {},
   "source": [
    "<a id='Def.MetodoMaximaVerosimilitud'></a>\n",
    "#### **Método de máxima verosimilitud**\n",
    "\n",
    "##### Ejemplo\n",
    "Un estanque es visitado diariamente por patos para actividades de alimentación, reproducción y esparcimiento. Se asume que el número de patos que visitan el estanque en un día cualquiera, $X$, sigue una distribución Pisson ($X \\sim P(\\theta)$), y es de interés para los biólogos conocer el valor del parámetro.\n",
    "\n",
    "Supoga que contratan a un estadístico muy perezoso y éste solo toma una observación. Va un día y observa que hay $x=3$ patos. ¿Qué valor sería razonable darle al parámetro y porque? \n",
    "\n",
    "> Sin ningun rigor aún podriamos pensar que asignarle un valor a $\\lambda$ en una variable aleatoria Poisson de la cual tenemos como muestra aleatoria, unicamente la misma observación; sería el mismo valor de $\\hat{\\lambda}_{an} = 3$ dado que tenemos que $E[X]=\\lambda$ (Esta sería una estimación por analogía).\n",
    "\n",
    "> Aunque no es un método del todo eficaz dado si intentamos estimar el parámetro de $\\lambda$ haciendo uso del hecho que $\\operatorname{Var}(X) = \\lambda$ tenemos que $S_1^2=\\frac{1}{1-1}\\sum_{i=1}^{1} (X_i-\\overline{X})^2$ es una indeterminación.\n",
    "\n",
    "> Si intenamos estimar por el método de los momentos, haciendo uso del primer momento tambien tenemos que $\\hat{\\lambda}=\\overline{X}$ y de este modo $\\hat{\\lambda}_{mom}=3$\n",
    "\n",
    "El estadístico perezoso entonces para justificar su labor vio que, la [densidad de una variable aleatoria Poisson](NotasDeClase1_ConceptosDeProbabilidad.ipynb//#DistribuciónPoisson) de $\\lambda=1,2,3,4$ son las siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df6645e-cb54-4ef4-ac10-c1122ce8371d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide-input\",\n",
    "    ]\n",
    "}\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#Calculate the first four moments:\n",
    "mu = 1\n",
    "mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n",
    "\n",
    "#Display the probability mass function (pmf):\n",
    "x = np.arange(poisson.ppf(0.01, mu),\n",
    "      poisson.ppf(0.99, mu))\n",
    "ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n",
    "ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n",
    "#Alternatively, the distribution object can be called (as a function) to fix the shape and location. This returns a “frozen” RV object holding the given parameters fixed.\n",
    "\n",
    "#Freeze the distribution and display the frozen pmf:\n",
    "rv = poisson(mu)\n",
    "ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
    "        label='frozen pmf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba92216c-df85-4223-b762-56f309139c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide-input\",\n",
    "    ]\n",
    "}\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#Calculate the first four moments:\n",
    "mu = 2\n",
    "mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n",
    "\n",
    "#Display the probability mass function (pmf):\n",
    "x = np.arange(poisson.ppf(0.01, mu),\n",
    "              poisson.ppf(0.99, mu))\n",
    "ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n",
    "ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n",
    "#Alternatively, the distribution object can be called (as a function) to fix the shape and location. This returns a “frozen” RV object holding the given parameters fixed.\n",
    "\n",
    "#Freeze the distribution and display the frozen pmf:\n",
    "rv = poisson(mu)\n",
    "ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
    "        label='frozen pmf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ad11e6-2e78-41b0-98a6-a5c2eff2bb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide-input\",\n",
    "    ]\n",
    "}\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#Calculate the first four moments:\n",
    "mu = 3\n",
    "mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n",
    "\n",
    "#Display the probability mass function (pmf):\n",
    "x = np.arange(poisson.ppf(0.01, mu),\n",
    "              poisson.ppf(0.99, mu))\n",
    "ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n",
    "ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n",
    "#Alternatively, the distribution object can be called (as a function) to fix the shape and location. This returns a “frozen” RV object holding the given parameters fixed.\n",
    "\n",
    "#Freeze the distribution and display the frozen pmf:\n",
    "rv = poisson(mu)\n",
    "ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
    "        label='frozen pmf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1324dea-5b5d-421b-b59e-bac41b66622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide-input\",\n",
    "    ]\n",
    "}\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#Calculate the first four moments:\n",
    "mu = 4\n",
    "mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n",
    "\n",
    "#Display the probability mass function (pmf):\n",
    "x = np.arange(poisson.ppf(0.01, mu),\n",
    "              poisson.ppf(0.99, mu))\n",
    "ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n",
    "ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n",
    "#Alternatively, the distribution object can be called (as a function) to fix the shape and location. This returns a “frozen” RV object holding the given parameters fixed.\n",
    "\n",
    "#Freeze the distribution and display the frozen pmf:\n",
    "rv = poisson(mu)\n",
    "ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
    "        label='frozen pmf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ba099-057f-49ea-b806-0c40197e0553",
   "metadata": {},
   "source": [
    "y por lo tanto el valor que de $\\lambda$ que maximiza el hecho de que se tengan más observaciones de $3$ patos en el estanque es el valor de $\\lambda=3$.\n",
    "\n",
    "> Por lo tanto la ideal de la estimación por máxima verosimilitud es encontrar los valores de los parámetros que maximizan el \"el chance\" de que mis datos sean observados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be2868-e8b4-4108-87a5-114e1ac6e11e",
   "metadata": {},
   "source": [
    "<a id='Def.FuncionVerosimilitud'></a>\n",
    "##### **Función de verosimilitud**\n",
    "Es una función del vector de parámetros $\\vec{\\theta}$ relacionada con la función de densidad conjunta de las obsersevaciones $\\mathbf{x}$ de la muestra mediante la siguiente expresión:\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}|\\mathbf{x}) = \n",
    "\\begin{cases}\n",
    " f_{\\mathbf{X}}(\\vec{\\theta};\\mathbf{x}) = f_{\\mathbf{X}}(x_1,x_2,\\cdots,x_n;\\vec{\\theta})   & \\text{si } \\mathbf{X} \\text{ es continuo} \\\\\n",
    " p_{\\mathbf{X}}(\\vec{\\theta};\\mathbf{x}) = p_{\\mathbf{X}}(x_1,x_2,\\cdots,x_n;\\vec{\\theta})   & \\text{si } \\mathbf{X} \\text{ es discreto}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "donde $f_{X}$ es la función de densidad multivariada de la variable aleatoria $\\mathbf{X}$ pero en este caso las obsevaciones son constantes. Lo mismo para la función de masa $p_X$.\n",
    "\n",
    "<a id='Def.MLE'></a>\n",
    "##### **Método de máxima verosimilitud**\n",
    "Se define como estimador de máxima verosimilitud o MLE (*Maximum likehood estimation*) a la estadística $\\vec{\\hat{\\theta}}(\\mathbf{X})$ que satisface:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\vec{\\hat{\\theta}}(\\mathbf{X}) \n",
    "&=\\underset{\\vec{\\theta} \\in \\Theta}{\\operatorname{arg max}} L(\\hat{\\theta} | \\mathbf{x}) \\\\\n",
    "&=\\underset{\\vec{\\theta} \\in \\Theta}{\\operatorname{arg max}} \\ln L(\\hat{\\theta} | \\mathbf{x}) =: l(\\hat{\\theta} | \\mathbf{x})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "donde $l(\\hat{\\theta} | \\mathbf{x})$ el la log-verosimilitud.\n",
    "\n",
    "> Tenemos que maximizar a $f$ es lo mismo que maximizar a $\\log f$ dado que $\\log$ es una función monótona creciente y \n",
    "\n",
    "> Podemos decir que en la función de máxima verosimilitud no necesariamente se cumple que $0 \\leq L(\\vec{\\theta} | \\mathbf{X}) \\leq 1$ dado que para el caso de las funciones de masa de probabilidad conjunta $p_X$ si tenemos valores entre 0 y 1 ya que para cada $p_{X_i}$  tambien tenemos valores entre 0 y 1 y es su producto esto se mantiene. Pero en el caso de función de distribución de probabilidad $f_X(x_1,x_2,\\cdots,x_n;\\vec{\\theta})$ es simplemente un número real no negativo, lo que si tenemos es que su integral tiene que ser 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce68f5-fc5d-448d-bc27-80d545e4014a",
   "metadata": {},
   "source": [
    "<a id='EjemploMLEBer'></a>\n",
    "###### Ejemplo\n",
    "Sea $X_1,X_2,\\cdots,X_n$ una muestra aleatoria tal que $X_1 \\sim Ber(\\theta)$. Determine el MLE y el estimador por momentos del parámetro $\\theta$\n",
    "\n",
    "Primero escribiremos la función de verosimilitud, como tenemos que $X_i$ es una variable aleatoria discreta tal que su función de masa de probabilidad es\n",
    "\n",
    "$$\n",
    "P_{x_i}(x;\\theta)=\\theta^{x_i}(1-\\theta)^{1-x_i} I_{\\{0,1\\}}(x_i)\n",
    "$$\n",
    "\n",
    "para toda $i$ de la [muestra aleatoria (m.a.)](NotasDeClase1_ConceptosDeProbabilidad.ipynb/#DefinicionMuestraAleatoria) y donde el espacio del parámetro es $\\Theta=[0,1]$. De este modo si la función de masa de probabilidad conjunta de la variables aleatorias es tal que $\\theta \\in (0,1)$ entonces\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta | x_1, x_2, \\cdots, x_n) \n",
    "& = p_{x_1,x_2,\\cdots,x_n}(x_1,x_2,\\cdots,x_n;\\theta) \\\\\n",
    "& \\overset{\\text{m.a.}}{=} \\prod_{i=1}^{n} P_{x_i}(x;\\theta) \\\\\n",
    "& = \\prod_{i=1}^{n} \\theta^{x_i}(1-\\theta)^{1-x_i} I_{\\{0,1\\}}(x_i) \\\\\n",
    "& = \\theta^{\\sum_{i=1}^{n} x_i} (1 - \\theta)^{n-\\sum_{i=1}^{n} x_i } \\prod_{i=1}^{n} I_{\\{0,1\\}}(x_i)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Si $\\theta = 0,1$ tenemos que\n",
    "\n",
    "$$\n",
    "P_{x_i}(x;\\theta)=\n",
    "\\begin{cases}\n",
    "P[X=0] & x=0 \\\\\n",
    "P[X=1] & x=1\n",
    "\\end{cases}\n",
    "=\n",
    "\\begin{cases}\n",
    "\\theta & x=1 \\\\\n",
    "1- \\theta & x=0\\\\\n",
    "0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "y de este modo\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta | x_1, x_2, \\cdots, x_n) \n",
    "& = p_{x_1,x_2,\\cdots,x_n}(x_1,x_2,\\cdots, x_n; \\theta) \\\\\n",
    "& \\overset{\\text{m.a.}}{=} \n",
    "    \\begin{cases} \n",
    "        \\prod_{i=1}^{n} \\theta & \\theta=1 \\\\\n",
    "        \\prod_{i=1}^{n} 1-\\theta & \\theta=0 \\\\\n",
    "        0 & \\text{en otro caso}\n",
    "    \\end{cases} \\\\\n",
    "& =\n",
    "    \\begin{cases} \n",
    "        \\theta^n & \\theta=1 \\\\\n",
    "        (1-\\theta)^n & \\theta=0 \\\\\n",
    "        0 & \\text{en otro caso}\n",
    "    \\end{cases} \\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac8372-d4c6-4829-a69d-2dd4f25db2fe",
   "metadata": {},
   "source": [
    "Ahora vemos que como tenemos varios exponentes y productos, es conveniente maximizar mejor la función log similitud, de este modo tenemos que\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "l(\\theta | x_1, x_2, \\cdots, x_n) \n",
    "& =\\ln{[ p_{x_1,x_2,\\cdots,x_n}(x_1,x_2,\\cdots,x_n;\\theta) ]} \\\\\n",
    "& = \\ln{\\left[ \\theta^{\\sum_{i=1}^{n} x_i} (1 - \\theta)^{n-\\sum_{i=1}^{n} x_i } \\prod_{i=1}^{n} I_{\\{0,1\\}}(x_i) \\right]}\\\\\n",
    "& = \\ln{ \\theta^{\\sum_{i=1}^{n} x_i} } + \\ln{ (1 - \\theta)^{n-\\sum_{i=1}^{n} x_i} } + \\ln{ \\prod_{i=1}^{n} I_{\\{0,1\\}}(x_i) } \\\\\n",
    "& = ({\\sum_{i=1}^{n} x_i} )\\ln{ \\theta } + ({n-\\sum_{i=1}^{n} x_i})\\ln{ (1 - \\theta) } + \\ln{ \\prod_{i=1}^{n} I_{\\{0,1\\}}(x_i) } \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Vamos a empezar a estudiar el $int(\\Theta) = (0,1)$ y necesitamos ver $\\underset{\\vec{\\theta} \\in \\Theta}{\\operatorname{arg max}} l(\\hat{\\theta} | \\mathbf{x})$ (el argumento $\\theta$ que maximiza la log-similitud) de este modo tenemos que encontrar los puntos estacionarios $\\theta^*$ tales que $\\frac{d l}{d\\theta}(\\theta^*) = 0$ entoces\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{d l}{d \\theta}(\\theta) \n",
    "& = \\frac{d}{d \\theta} \\left( ({\\sum_{i=1}^{n} x_i} )\\ln{ \\theta } + ({n-\\sum_{i=1}^{n} x_i})\\ln{ (1 - \\theta) } + \\ln{ \\prod_{i=1}^{n} I_{\\{0,1\\}}(x_i) } \\right) \\\\\n",
    "& =  \\frac{\\sum_{i=1}^{n} x_i}{\\theta} - \\frac{n-\\sum_{i=1}^{n} x_i}{1-\\theta}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "donde igualando a cero tenemos\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "0 & = \\frac{\\sum_{i=1}^{n} x_i}{\\theta^*} - \\frac{n-\\sum_{i=1}^{n} x_i}{1-\\theta^*} , \\theta \\neq 0,1\\\\\n",
    "\\frac{\\sum_{i=1}^{n} x_i}{\\theta^*} & = \\frac{n-\\sum_{i=1}^{n} x_i}{1-\\theta^*} \\\\\n",
    "(1-\\theta^*)\\frac{\\sum_{i=1}^{n} x_i}{\\theta^*} & = n-\\sum_{i=1}^{n} x_i \\quad  \\\\\n",
    "& \\vdots \\\\\n",
    "\\theta^* & = \\frac{ \\sum_{i=1}^{n} x_i }{ n } = \\overline{X}_n\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Teniendo en cuenta la definición de la [media muestral](#DefinicionMediaMuestral) $\\overline{X}_n$ tenemos que para $X_i \\sim Ber(\\theta)$, $0 \\leq \\overline{X} \\leq 1 $ (c.s.) tenemos que restringir de la siguiente forma $0 \\leq \\overline{X} \\leq 1 $, para que  $ \\theta \\in (0,1) = Int(\\Theta)$ \n",
    "\n",
    "Ahora vimos que $\\overline{X}_n$ es un punto estacionario de la función de log-similitud $l$, de este modo para saber si tenemos un máximo o un mínimo , local o global, tenemos que desarrollar $\\frac{d^2l}{d\\theta^2}$ y ver si $\\frac{d^2l}{d\\theta^2}(\\theta^*) < 0$ para saber que la función es estrictamente cóncava y asi determinar que $\\theta^*=\\overline{X}_n$ es un máximo global. \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{d^2 l}{d \\theta^2}(\\theta) \n",
    "& =  \\frac{\\sum_{i=1}^{n} x_i}{\\theta} - \\frac{n-\\sum_{i=1}^{n} x_i}{1-\\theta}\\\\\n",
    "& =  \\frac{-\\sum_{i=1}^{n} x_i}{\\theta^2} - \\frac{n-\\sum_{i=1}^{n} x_i}{(1-\\theta)^2}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Donde evaluando $\\theta^*=\\overline{X}_n$ tenemos que \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\frac{-\\sum_{i=1}^{n} x_i}{(\\overline{X}_n)^2} - \\frac{n-\\sum_{i=1}^{n} x_i}{(1-\\overline{X}_n)^2} \\\\\n",
    "& = \\frac{-\\frac{n}{n}\\sum_{i=1}^{n} x_i}{(\\overline{X}_n)^2} - \\frac{\\frac{n}{n}(n-\\sum_{i=1}^{n} x_i)}{(1-\\overline{X}_n)^2} \\\\\n",
    "& = \\frac{-n\\frac{\\sum_{i=1}^{n} x_i}{n}}{(\\overline{X}_n)^2} - \\frac{n(1-\\frac{\\sum_{i=1}^{n} x_i}{n})}{(1-\\overline{X}_n)^2} \\\\\n",
    "& = \\frac{-n\\overline{X}_n}{(\\overline{X}_n)^2} - \\frac{n(1-\\overline{X}_n)}{(1-\\overline{X}_n)^2} \\\\\n",
    "& = \\frac{-n}{\\overline{X}_n} - \\frac{n}{1-\\overline{X}_n} < 0\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "y de este modo denemos que $l$ es estrictamente cóncava y por tanto con máximo global en $\\theta^*=\\hat{\\theta}=\\overline{X}_n$\n",
    "\n",
    "Finalmente tenemos que ver que sucede cuando $\\overline{X}_n = 0,1$ recordadno que entonces tuvimos sólo éxitos o fracasos correspondientemente y además que para cuando $\\theta \\in \\{ 0,1 \\}$\n",
    "\n",
    "$$\n",
    "L(\\theta,\\mathbf{x}) =\n",
    "\\begin{cases} \n",
    "        \\theta^n & x=1 \\\\\n",
    "        (1-\\theta)^n & x=0 \\\\\n",
    "        0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "por lo tanto tenemos que para $\\theta \\in \\Theta$ que \n",
    "\n",
    "$$\n",
    "L(\\theta,\\mathbf{x}) =\n",
    "\\begin{cases} \n",
    "        \\theta^n & \\theta=1 \\\\\n",
    "        \\theta^{\\sum_{i=1}^{n} x_i} (1 - \\theta)^{n-\\sum_{i=1}^{n} x_i } \\prod_{i=1}^{n} I_{\\{0,1\\}}(x_i)  & 0<\\theta<1\\\\\n",
    "        (1-\\theta)^n & \\theta=0 \\\\\n",
    "        0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "y de este modo\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{MLE} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{Si } \\overline{X}_n=1 \\\\\n",
    "    \\overline{X}_n & \\text{Si } 0<\\overline{X}<1 \\\\\n",
    "    0 & \\text{Si } \\overline{X}_n=0 \\\\\n",
    "\\end{cases} = \n",
    "\\overline{X}_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea04ff-f4eb-4276-a0b5-8787847533ad",
   "metadata": {},
   "source": [
    "###### Ejemplo\n",
    "Sea $X_1,X_2,\\cdots,X_n$ una muestra aleatoria. Determine el MLE del parámetro $\\theta$ se la m.a. tiene función de densidad.\n",
    "\n",
    "$$\n",
    "f_X(x;\\theta) \n",
    "= I_{\\{\\theta-\\frac{1}{2},\\theta+\\frac{1}{2}\\}}(x) \n",
    "= \\begin{cases}1 & \\theta-\\frac{1}{2} \\leq x \\leq \\theta + \\frac{1}{2} \\\\ 0 & \\text{e.o.c.}\\end{cases}\n",
    "$$\n",
    "\n",
    "Con la siguiente representación se puede ver como se podria verificar que $f_X$ es una autentica función de desidad, tambien que el espacio del parámetro $\\Theta = \\mathbb{R}$. \n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/O-sUO5x0RaXDNdzKObjoO8aFytYe1rNYSyNQASjjlGA.original.fullsize.png)\n",
    "\n",
    "Tambien podemos ver que $X_i \\sim U[\\theta-\\frac{1}{2},\\theta + \\frac{1}{2}]$, es este modo si intentamos encontrar el estimado máximo verosimil haciendo uso de la función de verosimilitud tendriamos que\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta|\\mathbf{x})\n",
    "& = \\prod_{i=1}^{n} f_{x_i}(x_i, \\theta) \\\\\n",
    "& = \\prod_{i=1}^{n} I_{\\{\\theta-\\frac{1}{2},\\theta+\\frac{1}{2}\\}}(x_i) \\\\\n",
    "& = \\begin{cases} 1 &  \\text{Si } \\theta -\\frac{1}{2} \\leq x_1,x_2,\\cdots,x_n \\leq \\theta +\\frac{1}{2}\\\\\n",
    "                  0 & \\text{e.o.c.}\\end{cases} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "De este modo tenemos que $L(\\theta|\\mathbf{x})=1$ siempre y cuando $\\theta -\\frac{1}{2} \\leq x_1,x_2,\\cdots,x_n \\leq \\theta +\\frac{1}{2}$ para todo $1\\leq i \\leq n$. En este punto teniendo en cuenta que nuestro parámetro se encuentra entre la condición, podemos ver que no podemos encontrar un método de maximización usando las propiedades de optimización de funciones del cálculo.\n",
    "\n",
    "Haciendo una proximación diferente veamos que \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\theta - \\frac{1}{2} \\leq x_i \\leq \\theta + \\frac{1}{2} \\quad \\forall i \\\\\n",
    "\\iff \\\\ \n",
    "x_i \\geq \\theta - \\frac{1}{2} \\ \\land \\ x_i \\leq \\theta + \\frac{1}{2} \\quad \\forall i \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Y teniendo en cuenta la definición de [estadísticas de orden](#DefinicionEstadisticasOrden) podemos ver que $X^{(1)} \\geq \\theta - \\frac{1}{2}$ y tambien $X^{(n)} \\leq \\theta + \\frac{1}{2}$. Por tanto $\\theta \\leq X^{(1)} + \\frac{1}{2}$ y $\\theta \\geq X^{(n)} - \\frac{1}{2}$. En palabras tenemos que $\\theta$ tiene que ser menor al mínimo de la muestra mas $\\frac{1}{2}$ y además $\\theta$ tiene que ser mayor al máximo menos $\\frac{1}{2}$. Así pues cualquier valor de $\\theta$ entre $[X^{(n)} - \\frac{1}{2} , X^{(1)} + \\frac{1}{2}]$ hace máxima a la verosimilitud.\n",
    "\n",
    "Si queremos ver la función de verosimilitud trabajada podemos decir que $L(\\theta | \\mathbf{x}) = I_{\\{ X^{(n)} - \\frac{1}{2} , X^{(1)} + \\frac{1}{2} \\}} (\\theta)$, lo que gráficamente es \n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/O-sUO5x0RaXDNdzKObjoO8aFytYe1rNYSyNQASjjlGA.original.fullsize.png)\n",
    "\n",
    "y podemos ver que el valor de $\\hat{\\theta}_{\\text{MLE}}$ no es único. Ya que cualquier solución en la muestra entre $X^{(n)} - \\frac{1}{2}$ y $X^{(1)} + \\frac{1}{2}$ es un MLE (maximum likehood estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ebdbd-a578-4145-8fa6-21bf447eba54",
   "metadata": {},
   "source": [
    "<a id='EjemploEstimador1'></a>\n",
    "###### Ejemplo\n",
    "Sea $X_1,X_2,\\cdots,X_n$ un muestra aleatoria. Determine el MLE y el estimador de momentos del parámetro $\\theta$ si la muestra aletoria tiene función de densidad a\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "f_X(x;\\theta) \n",
    "& = \\frac{1}{\\theta}I_{(0,\\theta)} (x) \\\\\n",
    "& = \\begin{cases}\n",
    "        \\frac{1}{\\theta} & \\text{Si } x \\in (0,\\theta) \\\\\n",
    "        0 & \\text{e.o.c} \\\\\n",
    "    \\end{cases}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Primero veamos que efectivamente tenemos una función de densidad de probabilidad. En principio vemos que $f_X$ está definida para todos los reales siempre  que los valores de $\\theta > 0$. Tambien bajo la misma condición de $\\theta$ tenemos que $f_X$ es una función no negativa y ademas su integral sobre los $\\mathbb{R}$ es 1. Veamos la gráfica de esta función para notar esto.\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/O-sUO5x0RaXDNdzKObjoO8aFytYe1rNYSyNQASjjlGA.original.fullsize.png)\n",
    "\n",
    "Nuevamente podemos ver que nuestro parámetro, el cual vimos que su soporte es tal que $\\Theta=(0,\\theta)$  además de encontrarse en la definición de la función de densidad, donde $X_i \\sim U(0,\\theta)$, se encuentra sobre el soporte de la variable aletoria. Definiendo la función de verosimilitud al igual que el ejemplo anterior tenemos\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta | \\mathbf{x}) \n",
    "& \\overset{\\text{m.a.}}{=} \\prod_{i=1}^{n} f_X(x_i;\\theta) \\\\\n",
    "& = \\prod_{i=1}^{n} \\frac{1}{\\theta} I_{(0,\\theta)} \\\\\n",
    "& = \\left( \\frac{1}{\\theta} \\right)^n \\prod_{i=1}^{n} I_{(0,\\theta)} \\\\\n",
    "& = \\begin{cases}\n",
    "        \\frac{1}{\\theta^n} & \\text{Si } 0 < x_1,x_2,\\cdots,x_n < \\theta \\\\\n",
    "        0 & \\text{e.o.c.} \\\\\n",
    "    \\end{cases}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Lo que estamos diciendo es que $\\prod_{i=1}^{n} I_{(0,\\theta)}(x_i) =1$ lo que equivale a que $0 < x_i$ para todo $1 \\leq i \\leq n$ y además $x_i < \\theta$ para todo $1 \\leq i \\leq n$. Como sólo en la segunda desigualdad tenemos información sobre $\\theta$ vemos que $\\theta > X^{(n)}$. De este modo tenemos podemos formular una nueva indicadora que no tenga la una restricción en función de $\\theta$ la cual es $I_{(X^{(n)},\\infty)}(\\theta) = 1$. Así tenemos que la función de verosimilitu está definida de la siguiente manera\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta|\\mathbf{x}) \n",
    "& = \\frac{1}{\\theta^n}I_{(X^{(n)},\\infty)}(\\theta) \\\\\n",
    "& = \\begin{cases}\n",
    "        \\frac{1}{\\theta^n} & \\text{Si } \\theta > X^{(n)} \\\\\n",
    "        0 & \\text{Si } \\theta \\leq X^{(n)} \\\\\n",
    "    \\end{cases}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "De este modo podemos ver que la gráfica de nuestra función de verosimilitud es\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/EntgPpfmXrSLTUbN6H3RTIjbpNibvMa7YgncLYS7E3A.original.fullsize.png)\n",
    "\n",
    "Para encontrar $\\underset{\\theta \\in (0,\\infty)}{\\operatorname{arg max}} L(\\theta,\\mathbf{x})$ tengamos en cuenta que en la definición de maximo global el $x^* \\in U$ tal que $ f(x^*) \\geq f(x)$ para toda $x \\in U$.\n",
    "De este modo no tendremos un máximo global dado que siempre vamos a encontrar un número entre alguno que tomemos entre $X^{(n)}$ y a la derecha de $X^{(n)}$.  \n",
    "\n",
    "Para este caso tendremos que ver al tomar $\\underset{\\theta \\in (0,\\infty)}{\\operatorname{arg sup}} L(\\theta,\\mathbf{x})$ (el supremo de L, es decir la mínima cota superior de la función L ($x^* \\in U$ es el $\\operatorname{arg sup}$ si $\\lim_{x \\to x^{*}(-+)} f(x) \\geq f(x) \\quad \\forall x \\in U $ )) de este modo tenemos que $\\hat{\\theta}_{\\text{MLE}} = \\underset{\\theta \\in (0,\\infty)}{\\operatorname{arg sup}} = L(\\theta,\\mathbf{x})=X^{(n)}$\n",
    "\n",
    "\n",
    "Estimando ahora por el método de los momentos tenemos que $\\mu_1 = E[X^1] = E[X]$ donde tambien $\\overset{\\sim}{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\overline{X}_n$. Teniendo en cuenta que si $X \\sim U(0,\\theta)$ entonces $E[X] = \\frac{1}{2} (\\theta + 0) = \\frac{\\theta}{2} = \\mu_1(\\theta)$ de este modo tenemos que $\\frac{\\hat{\\theta}}{2} = \\overline{X}_n$ es decir que $\\hat{\\theta}_{\\text{mom}} = 2\\overline{X}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b49eb3-d4c8-4dfa-aee9-68b104ce45c0",
   "metadata": {},
   "source": [
    "###### Ejemplo\n",
    "Sea $X_1,X_2,\\cdots,X_n$ una muestra aleatoria con función de densidad de probabilidad Cauchy de la siguiente manera\n",
    "\n",
    "$$\n",
    "f_X(x;\\theta)=\\frac{1}{\\pi\\gamma(1+(\\frac{x-\\theta}{\\gamma})^2)}=\\frac{1}{\\pi(1+(x-\\theta)^2)}\n",
    "$$\n",
    "\n",
    "es decir que su parámetro de localizacion ($x_0$) es $\\theta$ y de escala ($\\gamma$) es 1. \n",
    "\n",
    "1) ¿Puede usarse el método de los momentos en este caso?\n",
    "2) Encuentre un estimador por el método de analogía\n",
    "3) Encuentre un estimador por el método de MLE\n",
    "\n",
    "*Solución 1:* Teniendo en cuenta que la [distribución de Cauchy](NotasDeClase1_ConceptosDeProbabilidad.ipynb#DistribucionCauchy) no tiene valor esperado, tenemos que no tiene su primer momento y en consecuencia no tiene los momentos siguientes y dado que tenemos dos parámetros a estimar, tendriamos que tener almenos los dos primeros momentos de la distribución para poder estimarlos por el método de los momentos. De este modo tenemos que no es posible estimar los parámetros por el método de los momentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d39b7f-ce6d-4492-b04d-16e609e02020",
   "metadata": {},
   "source": [
    "*Solución 2:* Para encontrar un estimador por el método de analogía notemos que la distribución de Cauchy  tiene como parámetro a la locación $x_0$ la cual es la mediana de los datos, de este modo el primer estimador por analogía de este parámetro es $\\mu_e$ la [mediana muestral](#DefinicionMedianaMuestral). Tambien para el caso de la localización un buen estimador es $\\mu_{\\text{moda}}$ la moda muestral de los datos. Así $\\hat{x_0} = \\mu_e$.\n",
    "\n",
    "Por otro lado para encontrar el parámetro $\\gamma$ de escala, a menudo es útil usar la mitad de [el rango intercuartílico muestral](#), es decir $\\hat{\\gamma} = X^{(\\lfloor n*0.75 \\rfloor)}-X^{(\\lfloor n*0.25 \\rfloor)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c3491-7b3d-4860-882b-389785a0df63",
   "metadata": {},
   "source": [
    "*Solución 3:* Para encontrar nuestro estimador por máxima verosimilitud veamos que el espacio del parámetro es $\\Theta = \\mathbb{R}$, luego $Int(\\Theta) = \\mathbb{R}$. Luego tenemos que \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta|\\mathbf{x}) \n",
    "& = \\prod_{i=1}^{n} f_X(\\theta|x_i) \\\\\n",
    "& = \\prod_{i=1}^{n} \\frac{1}{\\pi(1+(x_i-\\theta)^2)} \\\\\n",
    "& = \\frac{1}{\\pi^n}\\prod_{i=1}^{n} \\frac{1}{1+(x_i-\\theta)^2} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "donde tomando la log-similitud tenemos que \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\ln L(\\theta|\\mathbf{x}) \n",
    "& = \\ln \\frac{1}{\\pi^n}\\prod_{i=1}^{n} \\frac{1}{1+(x_i-\\theta)^2} \\\\\n",
    "& = \\ln \\frac{1}{\\pi^n} + \\sum_{i=1}^{n} \\ln \\frac{1}{1+(x_i-\\theta)^2} \\\\\n",
    "& = -n \\ln \\pi - \\sum_{i=1}^{n} \\ln (1+(x_i-\\theta)^2) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "De este modo derivando en función de $\\theta$ tenemos \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{dl}{d\\theta} \n",
    "& = \\frac{d}{d\\theta} \\left( -n \\ln \\pi - \\sum_{i=1}^{n} \\ln (1+(x_i-\\theta)^2) \\right) \\\\\n",
    "& = \\frac{d}{d\\theta} \\left( -\\sum_{i=1}^{n} \\ln (1+(x_i-\\theta)^2) \\right) \\\\\n",
    "& = -\\sum_{i=1}^{n} \\frac{d}{d\\theta} (\\ln (1+(x_i-\\theta)^2)) \\\\ \n",
    "& = \\sum_{i=1}^{n} \\frac{2(x_i-\\theta)}{1+(x_i-\\theta)^2} \\\\ \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Luego para encontrar nuestros puntos estacionarios tenemos que ver que $\\frac{dl}{d\\theta}(\\hat{\\theta})=0$, pero como vemos no podemos tener una solución analitica, tenemos que recurrir a métodos numéricos como **método de bisección** dado que estamos buscando raices o  **el ascenso del gradiente**.Por este hecho solo vamos a conocer estimaciones del parámetro es decir realizaciones de el estimador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac205d2-eb2d-414a-b973-df462426dc11",
   "metadata": {},
   "source": [
    "#### **Teorema Existencia y uncidad del MLE**\n",
    "Si $l(\\vec{\\theta}|\\mathbf{x})$ es diferenciable dos veces y estrictamente cóncava, y $l(\\vec{\\theta}|\\mathbf{x}) \\to -\\infty$ en el borde de $\\Theta$ (abierto); entonces $\\vec{\\hat{\\theta}}_{\\text{MLE}}$ existe y es único, y se puede encontrar resolviendo:\n",
    "\n",
    "$$\n",
    "\\nabla l (\\vec{\\hat{\\theta}}_{\\text{MLE}}|\\mathbf{x}) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "> Este teorema lo que nos dice es que para garantizar la existencia y la unicidad del estimado máximo verosímil se debe verificar que primero que la función de log-similitud es dos veces diferenciable y estrictamente cóncava. Tambien se debe verificar que en el borde del espacio del parámetro la función de log-similitud tienda a menos infinito, es decir que ella siga creciendo negativamente a menos que nos alejemos mucho. Así tendriamos que en la condiciones de primer orden, el punto encontrado sera único y el máximo de la función de log-similitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab94ed-77e7-4895-88cc-4ea32ee74b1f",
   "metadata": {},
   "source": [
    "#### **Teorema Invarianza del MLE**\n",
    "Si $\\hat{\\theta}(\\mathbf{X})$ es el MLE para $\\theta$ y $\\tau(.)$ es una función, el MLE para $\\tau(\\theta)$ será $\\tau(\\hat{\\theta}(\\mathbf{X}))$.\n",
    "\n",
    "##### Ejercicio\n",
    "Sea $X_1,X_2,\\cdots,X_n$ una muestra aleatoria. $Exp(\\theta)$. Determine el MLE para estimar la tasa promedio por unidad de tiempo de ocurrencia de fenómenos y el tiempo promedio entre fenómenos.\n",
    "\n",
    "Recordemos que $\\Theta = (0,\\infty)$, además $f_X(x;\\theta) = \\theta e^{-\\theta x} I_{(0,\\infty)}(x)$. Ademas tenemos que $E(X) =\\frac{1}{\\theta}$ se puede interpretar como el tiempo promedio entre fenómenos, por lo cual $\\theta$ es la tasa (rate) promedio de cuantes veces se tiene el fenómeno por unidad de tiempo; todo esto para un $X$:= como la duración del evento.\n",
    "\n",
    "Teniendo en cuenta esto vamos a escribir la función de verosimilitud de la densidad de probabilidad conjunta \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta | \\mathbf{X}) \n",
    "& = \\prod_{i=1}^{n} f_X(x_i;\\theta) \\\\\n",
    "& = \\prod_{i=1}^{n} \\theta e^{-\\theta x_i} I_{(0,\\infty)}(x_i) \\\\\n",
    "& = \\theta^n e^{\\theta \\sum_{n=1}^{n}x_i} \\prod_{i=1}^{n} I_{(0,\\infty)}(x_i)\\\\\n",
    "\\ln L(\\theta | \\mathbf{X})  & = \\ln \\theta^n e^{\\theta \\sum_{n=1}^{n}x_i} \\prod_{i=1}^{n} I_{(0,\\infty)}(x_i)\\\\\n",
    "l(\\theta | \\mathbf{X}) & = \\ln \\theta^n + \\ln e^{\\theta \\sum_{n=1}^{n}x_i}+ \\ln  \\prod_{i=1}^{n} I_{(0,\\infty)} \\\\\n",
    "& = n \\ln \\theta - \\theta \\sum_{n=1}^{n}x_i + \\ln  \\prod_{i=1}^{n} I_{(0,\\infty)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "De este modo tenemos que\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{dl}{d\\theta}(\\theta | \\mathbf{x}) \n",
    "    & = \\frac{d}{d\\theta}\\left(  n \\ln \\theta - \\theta \\sum_{n=1}^{n}x_i + \\ln  \\prod_{i=1}^{n} I_{(0,\\infty)} \\right) \\\\\n",
    "    & = \\frac{n}{\\theta} - \\sum_{n=1}^{n}x_i\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Y así\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{d^2l}{d\\theta^2}(\\theta | \\mathbf{x}) \n",
    "    & = \\frac{d}{d\\theta} \\left( \\frac{n}{\\theta} - \\sum_{n=1}^{n}x_i \\right) \\\\\n",
    "    & = \\frac{-n}{\\theta^2} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "de la cual tenemos que $\\frac{-n}{\\theta} < 0$  para todo $\\theta \\in \\Theta=(0,\\infty)$. De este modo recordando el teorema de existencia y unicidad del MLE tenemos la primera condición. Ahora tenemos que ver que sucede con la función de log-similitud en la frontera $\\partial \\Theta = \\{ 0, \\infty \\}$ esto es ver que pasa cuando \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\lim_{\\theta \\to 0^+} l(\\theta | \\mathbf{X}) \n",
    "& =\\lim_{\\theta \\to 0^+}  n\\ln \\theta - \\theta \\sum_{n=1}^{n}x_i + \\ln  \\prod_{i=1}^{n} I_{(0,\\infty)} \\\\\n",
    "& =\\lim_{\\theta \\to 0^+}  n\\ln \\theta - 0 + \\lim_{\\theta \\to 0^+}\\ln  \\prod_{i=1}^{n} I_{(0,\\infty)} \\\\\n",
    "& = ( \\lim_{\\theta \\to 0^+} n\\ln \\theta ) + \\ln  \\prod_{i=1}^{n} I_{(0,\\infty)} \\\\\n",
    "& = - \\infty +\\ln  \\prod_{i=1}^{n} \\\\\n",
    "& = - \\infty \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "y tambien cuando\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\lim_{\\theta \\to \\infty} l(\\theta | \\mathbf{X}) \n",
    "& =\\lim_{\\theta \\to \\infty}  n\\ln \\theta - \\theta \\sum_{n=1}^{n}x_i + \\ln  \\prod_{i=1}^{n} I_{(0,\\infty)} \\\\\n",
    "& = -\\infty\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Por lo tanto tenemos por el teorema de existencia y unicidad que $\\hat{\\theta}_{\\text{MLE}}$ existe y es único.\n",
    "\n",
    "Y así $\\frac{dl}{d\\theta}(\\hat{\\theta}_{\\text{MLE}}) = 0$ no dice el valor de único estimador por máxima verosimilitud.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{dl}{d\\theta}(\\hat{\\theta} | \\mathbf{x}) & = 0 \\\\\n",
    "    \\frac{n}{\\hat{\\theta}} & =  \\sum_{n=1}^{n}x_i  \\\\\n",
    "    \\hat{\\theta} & =  \\frac{n}{\\sum_{n=1}^{n}x_i} \\\\\n",
    "    \\hat{\\theta} & =  \\frac{1}{\\frac{1}{n}\\sum_{n=1}^{n}x_i} \\\\\n",
    "    \\hat{\\theta} & =  \\frac{1}{\\overline{X}_n} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Por último como tenemos qeu ver tambien el estimador para $\\frac{1}{\\theta}$ haciendo uso del teorema de invarianza tenemos que $g(\\hat{\\theta}) = (\\hat{\\theta})^{-1} = (\\frac{1}{\\overline{X}_n})^{-1} = \\overline{X}_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe8d70a-0ba3-4af8-aff0-086443d5821d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
